[cloudera@cdh6 ~]$ which spark-submit
/usr/bin/spark-submit
--submit由java,scala,pytho撰寫的application到spark cluster

--interactive interface使用REPL方式進行spark開發
--read - eval - print - loop
[cloudera@cdh6 ~]$ which pyspark         --python
/usr/bin/pyspark
[cloudera@cdh6 ~]$ which spark-shell     --scala
/usr/bin/spark-shell

[cloudera@cdh6 ~]$ ls -l /etc/spark/conf/
total 52
-rw-r--r-- 1 root root 27502 Sep 22 22:49 classpath.txt
-rw-r--r-- 1 root root    21 Sep 22 22:49 __cloudera_generation__
-rw-r--r-- 1 root root    67 Sep 22 22:49 __cloudera_metadata__進行
-rw-r--r-- 1 root root   951 Sep 22 22:49 log4j.properties
-rw-r--r-- 1 root root     0 Sep 22 22:49 navigator.lineage.client.properties
-rw-r--r-- 1 root root  1632 Sep 22 22:49 spark-defaults.conf
-rw-r--r-- 1 root root  2299 Sep 22 22:49 spark-env.sh
drwxr-xr-x 2 root root  4096 Sep 22 22:49 yarn-conf

[cloudera@cdh6 conf]$ cat spark-defaults.conf
spark.authenticate=false
spark.driver.log.dfsDir=/user/spark/driverLogs
spark.driver.log.persistToDfs.enabled=true
spark.dynamicAllocation.enabled=false
spark.dynamicAllocation.executorIdleTimeout=60
spark.dynamicAllocation.maxExecutors=2
spark.dynamicAllocation.minExecutors=0
spark.dynamicAllocation.schedulerBacklogTimeout=1
spark.eventLog.enabled=true
spark.io.encryption.enabled=false
spark.network.crypto.enabled=false
spark.serializer=org.apache.spark.serializer.KryoSerializer
spark.shuffle.service.enabled=false
spark.shuffle.service.port=7337
spark.ui.enabled=true
spark.ui.killEnabled=true
spark.lineage.log.dir=/var/log/spark/lineage
spark.lineage.enabled=true
spark.master=yarn
spark.submit.deployMode=client
spark.eventLog.dir=hdfs://cdh6:8020/user/spark/applicationHistory
spark.yarn.historyServer.address=http://cdh6:18088
spark.yarn.jars=local:/opt/cloudera/parcels/CDH-6.3.2-1.cdh6.3.2.p0.1605554/lib/spark/jars/*,local:/opt/cloudera/parcels/CDH-6.3.2-1.cdh6.3.2.p0.1605554/lib/spark/hive/*
spark.driver.extraLibraryPath=/opt/cloudera/parcels/CDH-6.3.2-1.cdh6.3.2.p0.1605554/lib/hadoop/lib/native
spark.executor.extraLibraryPath=/opt/cloudera/parcels/CDH-6.3.2-1.cdh6.3.2.p0.1605554/lib/hadoop/lib/native
spark.yarn.am.extraLibraryPath=/opt/cloudera/parcels/CDH-6.3.2-1.cdh6.3.2.p0.1605554/lib/hadoop/lib/native
spark.yarn.config.gatewayPath=/opt/cloudera/parcels
spark.yarn.config.replacementPath={{HADOOP_COMMON_HOME}}/../../..
spark.yarn.historyServer.allowTracking=true
spark.yarn.appMasterEnv.MKL_NUM_THREADS=1
spark.executorEnv.MKL_NUM_THREADS=1
spark.yarn.appMasterEnv.OPENBLAS_NUM_THREADS=1
spark.executorEnv.OPENBLAS_NUM_THREADS=1
spark.extraListeners=com.cloudera.spark.lineage.NavigatorAppListener
spark.sql.queryExecutionListeners=com.cloudera.spark.lineage.NavigatorQueryListener

[cloudera@cdh6 conf]$ cat spark-env.sh
#!/usr/bin/env bash
##
# Generated by Cloudera Manager and should not be modified directly
##

SELF="$(cd $(dirname $BASH_SOURCE) && pwd)"
if [ -z "$SPARK_CONF_DIR" ]; then
  export SPARK_CONF_DIR="$SELF"
fi

export SPARK_HOME=/opt/cloudera/parcels/CDH-6.3.2-1.cdh6.3.2.p0.1605554/lib/spark

SPARK_PYTHON_PATH=""
if [ -n "$SPARK_PYTHON_PATH" ]; then
  export PYTHONPATH="$PYTHONPATH:$SPARK_PYTHON_PATH"
fi

export HADOOP_HOME=/opt/cloudera/parcels/CDH-6.3.2-1.cdh6.3.2.p0.1605554/lib/hadoop
export HADOOP_COMMON_HOME="$HADOOP_HOME"

if [ -n "$HADOOP_HOME" ]; then
  LD_LIBRARY_PATH=$LD_LIBRARY_PATH:${HADOOP_HOME}/lib/native
fi

SPARK_EXTRA_LIB_PATH=""
if [ -n "$SPARK_EXTRA_LIB_PATH" ]; then
  LD_LIBRARY_PATH=$LD_LIBRARY_PATH:$SPARK_EXTRA_LIB_PATH
fi

export LD_LIBRARY_PATH

HADOOP_CONF_DIR=${HADOOP_CONF_DIR:-$SPARK_CONF_DIR/yarn-conf}
HIVE_CONF_DIR=${HIVE_CONF_DIR:-/etc/hive/conf}
if [ -d "$HIVE_CONF_DIR" ]; then
  HADOOP_CONF_DIR="$HADOOP_CONF_DIR:$HIVE_CONF_DIR"
fi
export HADOOP_CONF_DIR

PYLIB="$SPARK_HOME/python/lib"
if [ -f "$PYLIB/pyspark.zip" ]; then
  PYSPARK_ARCHIVES_PATH=
  for lib in "$PYLIB"/*.zip; do
    if [ -n "$PYSPARK_ARCHIVES_PATH" ]; then
      PYSPARK_ARCHIVES_PATH="$PYSPARK_ARCHIVES_PATH,local:$lib"
    else
      PYSPARK_ARCHIVES_PATH="local:$lib"
    fi
  done
  export PYSPARK_ARCHIVES_PATH
fi

if [ -f "$SELF/classpath.txt" ]; then
  export SPARK_DIST_CLASSPATH=$(paste -sd: "$SELF/classpath.txt")
fi

# Force single-threaded BLAS (CDH-58082).
export MKL_NUM_THREADS=${MKL_NUM_THREADS:-1}
export OPENBLAS_NUM_THREADS=${OPENBLAS_NUM_THREADS:-1}

# Spark uses `set -a` to export all variables created or modified in this
# script as env vars. We use a temporary variables to avoid env var name
# collisions.
# If PYSPARK_PYTHON is unset, set to CDH_PYTHON
TMP_PYSPARK_PYTHON=${PYSPARK_PYTHON:-''}
# If PYSPARK_DRIVER_PYTHON is unset, set to CDH_PYTHON
TMP_PYSPARK_DRIVER_PYTHON=${PYSPARK_DRIVER_PYTHON:-}

if [ -n "$TMP_PYSPARK_PYTHON" ] && [ -n "$TMP_PYSPARK_DRIVER_PYTHON" ]; then
  export PYSPARK_PYTHON="$TMP_PYSPARK_PYTHON"
  export PYSPARK_DRIVER_PYTHON="$TMP_PYSPARK_DRIVER_PYTHON"
fi

[cloudera@cdh6 ~]$ ls -l /etc/spark/conf/yarn-conf/
total 36
-rw-r--r-- 1 root root 3862 Sep 22 22:49 core-site.xml
-rw-r--r-- 1 root root  575 Sep 22 22:49 hadoop-env.sh
-rw-r--r-- 1 root root 1772 Sep 22 22:49 hdfs-site.xml
-rw-r--r-- 1 root root 5101 Sep 22 22:49 mapred-site.xml
-rw-r--r-- 1 root root  315 Sep 22 22:49 ssl-client.xml
-rw-r--r-- 1 root root  200 Sep 22 22:49 topology.map
-rw-r--r-- 1 root root 1626 Sep 22 22:49 topology.py
-rw-r--r-- 1 root root 3608 Sep 22 22:49 yarn-site.xml
[cloudera@cdh6 ~]$ cat /etc/spark/conf/yarn-conf/yarn-site.xml | grep resourcemanager.address -A 2 -B 1
  <property>
    <name>yarn.resourcemanager.address</name>
    <value>cdh6:8032</value>
  </property>

[cloudera@cdh6 ~]$ cat data.txt
Hello World              --以\n為結尾
Hello Hadoop
Hello Spark
[cloudera@cdh6 ~]$ hdfs dfs -put data.txt
[cloudera@cdh6 ~]$ hdfs dfs -ls /user/cloudera/data.txt
-rw-r--r--   1 cloudera cloudera         37 2019-09-24 18:31 /user/cloudera/data.txt

[cloudera@cdh6 ~]$ hdfs fsck /user/cloudera/data.txt -blocks
Connecting to namenode via http://cdh6:9870/fsck?ugi=cloudera&blocks=1&path=%2Fuser%2Fcloudera%2Fdata.txt
FSCK started by cloudera (auth:SIMPLE) from /127.0.0.1 for path /user/cloudera/data.txt at Tue Sep 24 20:11:32 PDT 2019
.Status: HEALTHY
 Total size:    37 B
 Total dirs:    0
 Total files:   1
 Total symlinks:                0
 Total blocks (validated):      1 (avg. block size 37 B)                --data.txt只有一個hdfs block
 Minimally replicated blocks:   1 (100.0 %)
 Over-replicated blocks:        0 (0.0 %)
 Under-replicated blocks:       0 (0.0 %)
 Mis-replicated blocks:         0 (0.0 %)
 Default replication factor:    1
 Average block replication:     1.0
 Corrupt blocks:                0
 Missing replicas:              0 (0.0 %)
 Number of data-nodes:          1
 Number of racks:               1
FSCK ended at Tue Sep 24 20:11:32 PDT 2019 in 1 milliseconds


The filesystem under path '/user/cloudera/data.txt' is HEALTHY

--pyspark則是啟動spark 2.4.0
--REPL(Read,Evaluate,Print,Loop)
[cloudera@cdh6 ~]$ pyspark
Python 2.7.5 (default, Apr  2 2020, 13:16:51)
[GCC 4.8.5 20150623 (Red Hat 4.8.5-39)] on linux2
Type "help", "copyright", "credits" or "license" for more information.
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /__ / .__/\_,_/_/ /_/\_\   version 2.4.0-cdh6.3.2
      /_/

Using Python version 2.7.5 (default, Apr  2 2020 13:16:51)
SparkSession available as 'spark'.

>>> sc    --core spark入口
<SparkContext master=yarn appName=PySparkShell>
>>> dir(sc)
['PACKAGE_EXTENSIONS', '__class__', '__delattr__', '__dict__', '__doc__', '__enter__', '__exit__', '__format__', '__getattribute__', '__getnewargs__', '__hash__', 
 '__init__', '__module__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__',
 '_accumulatorServer', '_active_spark_context', '_batchSize', '_callsite', '_checkpointFile', '_conf', '_dictToJavaMap', '_do_init', '_encryption_enabled',
 '_ensure_initialized', '_gateway', '_getJavaStorageLevel', '_initialize_context', '_javaAccumulator', '_jsc', '_jvm', '_lock', '_next_accum_id',
 '_pickled_broadcast_vars', '_python_includes', '_repr_html_', '_serialize_to_jvm', '_temp_dir', '_unbatched_serializer',
 'accumulator', 'addFile', 'addPyFile', 'appName', 'applicationId', 'binaryFiles', 'binaryRecords', 'broadcast', 'cancelAllJobs', 'cancelJobGroup',
 'defaultMinPartitions', 'defaultParallelism', 'dump_profiles', 'emptyRDD', 'environment', 'getConf', 'getLocalProperty', 'getOrCreate',
 'hadoopFile', 'hadoopRDD', 'master', 'newAPIHadoopFile', 'newAPIHadoopRDD', 'parallelize', 'pickleFile', 'profiler_collector', 'pythonExec', 'pythonVer',
 'range', 'runJob', 'sequenceFile', 'serializer', 'setCheckpointDir', 'setJobDescription', 'setJobGroup', 'setLocalProperty', 'setLogLevel', 'setSystemProperty',
 'show_profiles', 'sparkHome', 'sparkUser', 'startTime', 'statusTracker', 'stop', 'textFile', 'uiWebUrl', 'union', 'version', 'wholeTextFiles']

>>> sc.textFile("/user/cloudera/data.txt")
/user/cloudera/data.txt MapPartitionsRDD[1] at textFile at NativeMethodAccessorImpl.java:0

--將文字檔內容讀入,生成dataRDD
>>> dataRDD = sc.textFile("/user/cloudera/data.txt") 
--上面這個指令並沒有真正執行,因為spark是Lazy Execution,需要等到進行action操作時,才會真正執行

--對dataRDD的每個record進行函數操作,生成dataRDD_upper
>>> dataRDD_upper = dataRDD.map(lambda x: x.upper())

>>> dataRDD_upper.collect() --collect()要求將dataRDD_upper傳回給spark driver
[u'HELLO WORLD', u'HELLO HADOOP', u'HELLO SPARK']

>>> dataRDD_upper.getNumPartitions()
2  --即便data.txt只有一個HDFS block,預設spark cluster還是會將RDD分為2個partition

>>> help(sc)
 |  textFile(self, name, minPartitions=None, use_unicode=True)
 |      Read a text file from HDFS, a local file system (available on all
 |      nodes), or any Hadoop-supported file system URI, and return it as an
 |      RDD of Strings.
 |
 |      If use_unicode is False, the strings will be kept as `str` (encoding
 |      as `utf-8`), which is faster and smaller than unicode. (Added in
 |      Spark 1.2)
 |
 |      >>> path = os.path.join(tempdir, "sample-text.txt")
 |      >>> with open(path, "w") as testFile:
 |      ...    _ = testFile.write("Hello world!")
 |      >>> textFile = sc.textFile(path)
 |      >>> textFile.collect()
 |      [u'Hello world!']
 |

>>> sc.textFile("/user/cloudera/data.txt").map(lambda x: x.upper()).saveAsTextFile("/user/cloudera/dataupper")

------------------------------------------------------------------------------------------------------------------------
[cloudera@cdh6 ~]$ hdfs dfs -ls /user/cloudera/dataupper
Found 3 items
-rw-r--r--   1 cloudera supergroup          0 2020-09-16 09:39 /user/cloudera/dataupper/_SUCCESS
-rw-r--r--   1 cloudera supergroup         25 2020-09-16 09:39 /user/cloudera/dataupper/part-00000
-rw-r--r--   1 cloudera supergroup         12 2020-09-16 09:39 /user/cloudera/dataupper/part-00001
[cloudera@cdh6 ~]$ hdfs dfs -getmerge /user/cloudera/dataupper dataupper.txt
[cloudera@cdh6 ~]$ cat dataupper.txt
HELLO WORLD
HELLO HADOOP
HELLO SPARK
------------------------------------------------------------------------------------------------------------------------
>>> spark --sparkSQL入口
<pyspark.sql.session.SparkSession object at 0x7f676725fe50>

>>> dir(spark)
['Builder', '__class__', '__delattr__', '__dict__', '__doc__', '__enter__', '__exit__', '__format__', '__getattribute__', '__hash__', '__init__', '__module__',
 '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_convert_from_pandas',
 '_createFromLocal', '_createFromRDD', '_create_from_pandas_with_arrow', '_create_shell_session', '_get_numpy_record_dtype', '_inferSchema', '_inferSchemaFromList',
 '_instantiatedSession', '_jsc', '_jsparkSession', '_jvm', '_jwrapped', '_repr_html_', '_sc', '_wrapped', 'builder', 'catalog', 'conf', 'createDataFrame',
 'newSession', 'range', 'read', 'readStream', 'sparkContext', 'sql', 'stop', 'streams', 'table', 'udf', 'version']

>>> help(spark)
\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\
>>> dataDF=spark.read.text('/user/cloudera/data.txt')
>>> dataDF
DataFrame[value: string]
>>> dataDF.columns
['value']
>>> dir(dataDF)
['__class__', '__delattr__', '__dict__', '__doc__', '__format__', '__getattr__', '__getattribute__', '__getitem__', '__hash__', '__init__', '__module__', '__new__',
 '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_collectAsArrow', '_jcols', '_jdf', '_jmap',
 '_jseq', '_lazy_rdd', '_repr_html_', '_sc', '_schema', '_sort_cols', '_support_repr_html', 'agg', 'alias', 'approxQuantile', 'cache', 'checkpoint', 'coalesce',
 'colRegex', 'collect', 'columns', 'corr', 'count', 'cov', 'createGlobalTempView', 'createOrReplaceGlobalTempView', 'createOrReplaceTempView', 'createTempView',
 'crossJoin', 'crosstab', 'cube', 'describe', 'distinct', 'drop', 'dropDuplicates', 'drop_duplicates', 'dropna', 'dtypes', 'exceptAll', 'explain', 'fillna',
 'filter', 'first', 'foreach', 'foreachPartition', 'freqItems', 'groupBy', 'groupby', 'head', 'hint', 'intersect', 'intersectAll', 'isLocal', 'isStreaming',
 'is_cached', 'join', 'limit', 'localCheckpoint', 'na', 'orderBy', 'persist', 'printSchema', 'randomSplit', 'rdd', 'registerTempTable', 'repartition',
 'repartitionByRange', 'replace', 'rollup', 'sample', 'sampleBy', 'schema', 'select', 'selectExpr', 'show', 'sort', 'sortWithinPartitions', 'sql_ctx', 'stat',
 'storageLevel', 'subtract', 'summary', 'take', 'toDF', 'toJSON', 'toLocalIterator', 'toPandas', 'union', 'unionAll', 'unionByName', 'unpersist', 'where',
 'withColumn', 'withColumnRenamed', 'withWatermark', 'write', 'writeStream']

>>> dataDF.show()  --請注意show是DataFrame才有的action
+------------+
|       value|
+------------+
| Hello World|
|Hello Hadoop|
| Hello Spark|
+------------+
\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\
>>> data=sc.textFile('/user/cloudera/data.txt')

>>> data.collect() --將此rdd的內容從各個worker node傳送到spark driver並顯示出來
[u'Hello World', u'Hello Hadoop', u'Hello Spark']

>>> dataRDD.map(lambda x: x.upper())
PythonRDD[2] at RDD at PythonRDD.scala:53  --沒有執行,需要等到action操作,方開始執行

>>> dataRDD.map(lambda x: x.upper()).collect()
[u'HELLO WORLD', u'HELLO HADOOP', u'HELLO SPARK']

>>> dataRDD.count()
3 --此rdd裡有3個element,預設值文字檔裡以不可見換行符號\n為結尾當作一筆record

>>> data.getNumPartitions()
2 --此rdd分為2個partition(此處與mapreduce不同,cluster mode預設至少2個partition)

--map(func1) 一個element經過func1處理一次,輸出一個結果(下一個RDD的element)
                           單行匿名函數
                           ----------------------------						   
>>> dataRDD1 = dataRDD.map(lambda line: line.split(' '))
>>> dataRDD1.collect()
[[u'Hello',u'World'],
 [u'Hello',u'Hadoop'],
 [u'Hello',u'Spark']]

>>> dataRDD1.count
3

-- flatmap(func1) 一個element經過func1處理一次,輸出多個結果(每個結果都是下一個RDD的element)
>>> dataRDD1_flat = dataRDD.flatmap(lambda line: line.split(' '))
>>> dataRDD1.collect()
[u'Hello',
 u'World',
 u'Hello',
 u'Hadoop',
 u'Hello',
 u'Spark']
 
>>> dataRDD1.count()
6

>>> dataRDD1_kv = dataRDD_flat.map(lambda word: (word,1))  --Pair RDD表示此RDD的元素為tuple(key,value)
>>> dataRDD1_kv.collect() 
[(u'Hello',1),
 (u'World',1),
 (u'Hello',1),
 (u'Hadoop',1),
 (u'Hello',1),
 (u'Spark',1)]
 
    (Hello,1)
	(Hello,1)
	(Hello,1)
>>> dataRDD1_reduce = dataRDD_kv.reduceByKey(lambda v1,v2: v1+v2)  --Pair RDD表示此RDD的元素為tuple(key,value)
>>> dataRDD1_reduce.collect() 
[(u'Hadoop',1),
 (u'Hello',3),
 (u'Spark',1),
 (u'World',1)]

>>> print(dataRDD1_reduce.toDebugString())
(2) PythonRDD[39] at collect at <stdin>:1 []
 |  MapPartitionsRDD[38] at mapPartitions at PythonRDD.scala:374 []
 |  ShuffledRDD[37] at partitionBy at NativeMethodAccessorImpl.java:-2 []
 +-(2) PairwiseRDD[36] at reduceByKey at <stdin>:1 []
    |  PythonRDD[35] at reduceByKey at <stdin>:1 []
    |  /user/cloudera/data.txt MapPartitionsRDD[34] at textFile at NativeMethodAccessorImpl.java:-2 []
    |  /user/cloudera/data.txt HadoopRDD[33] at textFile at NativeMethodAccessorImpl.java:-2 []

>>> sc.textFile("/user/cloudera/data.txt").flatMap(lambda line: line.split(" ")).map(lambda word: (word,1)).reduceByKey(lambda v1, v2: v1 + v2).saveAsTextFile("/user/cloudera/wordcount")
---------------------------------------------------------------------------------------------------------------------------------------------------------
[cloudera@cdh6 ~]$ hdfs dfs -ls /user/cloudera/wordcount
Found 3 items
-rw-r--r--   1 cloudera supergroup          0 2020-09-16 10:24 /user/cloudera/wordcount/_SUCCESS
-rw-r--r--   1 cloudera supergroup         14 2020-09-16 10:24 /user/cloudera/wordcount/part-00000
-rw-r--r--   1 cloudera supergroup         43 2020-09-16 10:24 /user/cloudera/wordcount/part-00001
[cloudera@cdh6 ~]$ hdfs dfs -cat /user/cloudera/wordcount/part-00000
(u'Spark', 1)
[cloudera@cdh6 ~]$ hdfs dfs -cat /user/cloudera/wordcount/part-00001
(u'World', 1)
(u'Hello', 3)
(u'Hadoop', 1)
---------------------------------------------------------------------------------------------------------------------------------------------------------
--sc.textFile() 文字檔裡,以\n為結尾的一行,當作RDD的一個element
--sc.wholeTextFiles()
[cloudera@cdh6 ~]$ hdfs dfs -mkdir trnt
[cloudera@cdh6 ~]$ curl -O http://10.0.1.100:5050/Class_Log/trnt01.txt
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
  0    39    0    39    0     0   2802      0 --:--:-- --:--:-- --:--:--  4333
[cloudera@cdh6 ~]$ curl -O http://10.0.1.100:5050/Class_Log/trnt02.txt
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
  0    35    0    35    0     0   1530      0 --:--:-- --:--:-- --:--:--  5000
[cloudera@cdh6 ~]$ curl -O http://10.0.1.100:5050/Class_Log/trnt03.txt
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
  0    35    0    35    0     0   2834      0 --:--:-- --:--:-- --:--:--  4375
[cloudera@cdh6 ~]$ curl -O http://10.0.1.100:5050/Class_Log/trnt04.txt
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
106   639  106   639    0     0  41748      0 --:--:-- --:--:-- --:--:-- 63900

[cloudera@cdh6 ~]$ hdfs dfs -put trnt*.txt /user/cloudera/trnt/

[cloudera@cdh6 ~]$ hdfs dfs -ls /user/cloudera/trnt/
Found 4 items
-rw-r--r--   1 cloudera cloudera         39 2020-03-27 20:56 /user/cloudera/trnt/trnt01.txt
-rw-r--r--   1 cloudera cloudera         35 2020-03-27 20:56 /user/cloudera/trnt/trnt02.txt
-rw-r--r--   1 cloudera cloudera         35 2020-03-27 20:56 /user/cloudera/trnt/trnt03.txt
-rw-r--r--   1 cloudera cloudera        639 2020-03-27 21:03 /user/cloudera/trnt/trnt04.txt

[cloudera@cdh6 ~]$ pyspark
>>> trnt = sc.textFile('/user/cloudera/trnt/')
>>> trnt.collect()
[u'Two roads diverged in a yellow wood', u'', u'And sorry I could not travel both', u'And be one traveler, long I stood', u'And looked down one as far as I could', u'To where it bent in the undergrowth;', u'Then took the other, as just as fair,', u'And having perhaps the better claim,', u'Because it was grassy and wanted wear;', u'Though as for that the passing there', u'Had worn them really about the same,', u'And both that morning equally lay', u'In leaves no step had trodden black.', u'Oh, I kept the first for another day!', u'Yet knowing how way leads on to way,', u'I doubted if I should ever come back.', u'I shall be telling this with a sigh', u'Somewhere ages and ages hence:', u'Two roads diverged in a wood,and I??', u'I took the one less traveled by,', u'And that has made all the difference.']
>>> trnt.getNumPartitions()
5 --因為/user/cloudera/trnt/目錄下有4個hdfs file(每個file至少1個hdfs block),所以此rdd預設有4個partition

>>> trnt.count()
21

>>> trnt_wf=sc.wholeTextFiles('/user/cloudera/trnt/')
>>> trnt_wf.collect()
[(u'hdfs://cdh6:8020/user/cloudera/trnt/trnt01.txt', u'Two roads diverged in a yellow wood\r\n\r\n'), (u'hdfs://cdh6:8020/user/cloudera/trnt/trnt02.txt', u'And sorry I could not travel both\r\n'), (u'hdfs://cdh6:8020/user/cloudera/trnt/trnt03.txt', u'And be one traveler, long I stood\r\n'), (u'hdfs://cdh6:8020/user/cloudera/trnt/trnt04.txt', u'And looked down one as far as I could\r\nTo where it bent in the undergrowth;\r\nThen took the other, as just as fair,\r\nAnd having perhaps the better claim,\r\nBecause it was grassy and wanted wear;\r\nThough as for that the passing there\r\nHad worn them really about the same,\r\nAnd both that morning equally lay\r\nIn leaves no step had trodden black.\r\nOh, I kept the first for another day!\r\nYet knowing how way leads on to way,\r\nI doubted if I should ever come back.\r\nI shall be telling this with a sigh\r\nSomewhere ages and ages hence:\r\nTwo roads diverged in a wood,and I??\r\nI took the one less traveled by,\r\nAnd that has made all the difference.\r\n')]

>>> trnt_wf.take(1)
[(u'hdfs://cdh6:8020/user/cloudera/trnt/trnt01.txt', u'Two roads diverged in a yellow wood\r\n\r\n')]

>>> trnt_wf.count()
4
>>> trnt_wf.getNumPartitions()
1


[cloudera@cdh6 ~]$ cat frank.json
{
 "name":"frank",
 "job":"Instructor",
 "dept":"K100"
}
[cloudera@cdh6 ~]$ cat jack.json
{
 "name":"jack",
 "job":"PM",
 "dept":"K200"
}

[cloudera@cdh6 ~]$ hdfs dfs -mkdir hr
[cloudera@cdh6 ~]$ hdfs dfs -put frank.json /user/cloudera/hr/
[cloudera@cdh6 ~]$ hdfs dfs -put jack.json /user/cloudera/hr/
[cloudera@cdh6 ~]$ hdfs dfs -ls /user/cloudera/hr/
Found 2 items
-rw-r--r--   1 cloudera cloudera         57 2019-09-24 20:23 /user/cloudera/hr/frank.json
-rw-r--r--   1 cloudera cloudera         48 2019-09-24 20:24 /user/cloudera/hr/jack.json

[cloudera@cdh6 ~]$ pyspark
>>> people=sc.textFile('/user/cloudera/hr/')   --textFile()可以讀取一個目錄,此目錄下的所有檔案內容組成一個RDD
>>> people.collect()
[u'{', u' "name":"frank",', u' "job":"Instructor",', u' "dept":"K100"', u'}', u'{', u' "name":"jack",', u' "job":"PM",', u' "dept":"K200"', u'}']

>>> people.count()
10 --文字檔每行當作RDD的一個element

>>> people=sc.wholeTextFiles('/user/cloudera/hr/')
[(u'hdfs://cdh6:8020/user/cloudera/hr/frank.json', u'{\n "name":"frank",\n "job":"Instructor",\n "dept":"K100"\n}\n'), (u'hdfs://cdh6:8020/user/cloudera/hr/jack.json', u'{\n "name":"jack",\n "job":"PM",\n "dept":"K200"\n}\n')]
>>>> people.count()
2 --hr目錄下的每個file,成為一個element,但element為tuple型態(file name,file contents)
---------------------------------------------------------------------------------------------------------------------------------------------------------
--請先確認the_road_not_taken.txt在本地端已經存在
[cloudera@cdh6 ~]$ hdfs dfs -cat the_road_not_taken.txt
Two roads diverged in a yellow wood,
And sorry I could not travel both
And be one traveler, long I stood
And looked down one as far as I could
To where it bent in the undergrowth;
Then took the other, as just as fair,
And having perhaps the better claim,
Because it was grassy and wanted wear;
Though as for that the passing there
Had worn them really about the same,
And both that morning equally lay
In leaves no step had trodden black.
Oh, I kept the first for another day!
Yet knowing how way leads on to way,
I doubted if I should ever come back.
I shall be telling this with a sigh
Somewhere ages and ages hence:
Two roads diverged in a wood,and I??
I took the one less traveled by,
And that has made all the difference.

[cloudera@cdh6 ~]$ pyspark2
>>> rdd1=sc.textFile('the_road_not_taken.txt')  
#rdd1尚未產生,僅記錄rdd1的來源
>>> rdd1.count()
20 #count是一種action,所以導致rdd1真正產生並執行count操作,回傳結果為此rdd有幾個elements
   #rdd的平行處理單位為partition
>>> rdd1.getNumPartitions()
2  #回傳結果表示此rdd有幾個partition,每個partition可以有多個element組成

>>> rdd1.take(1)  --取1個element回傳給spark driver
[u'Two roads diverged in a yellow wood,']
>>> rdd1.take(2)  --取2個element回傳給spark driver
[u'Two roads diverged in a yellow wood,', u'And sorry I could not travel both']

>>> rdd1.glom().take(1) --取1個partition的element回傳給spark driver
[[u'Two roads diverged in a yellow wood,', u'And sorry I could not travel both', u'And be one traveler, long I stood', u'And looked down one as far as I could', u'To where it bent in the undergrowth;', u'Then took the other, as just as fair,', u'And having perhaps the better claim,', u'Because it was grassy and wanted wear;', u'Though as for that the passing there', u'Had worn them really about the same,']]

>>> rdd1.glom().take(2) --取2個partition的element回傳給spark driver

#map(function)將rdd1的每個element當作function輸入,而函數執行結果為rdd2的element
>>> rdd2=rdd1.map(lambda line: line.upper())  #使用lambda語法(匿名函數),因為函數結構簡單,可以使用一行完成
                         ----  ------------
						 輸入  輸出
\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\
>>> def upp(line):
...     return line.upper()
...
>>> rdd2=rdd1.map(upp)
\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\
>>> rdd2.take(2) #取rdd2的前兩個element傳給spark driver
[u'TWO ROADS DIVERGED IN A YELLOW WOOD,', u'AND SORRY I COULD NOT TRAVEL BOTH']

>>> rdd2.glom().take(2) #因為使用rdd2.glom(),所以操作單位變成partition. glom.take(2)取rdd2的前兩個partition的所有element傳給spark driver
[[u'TWO ROADS DIVERGED IN A YELLOW WOOD,', u'AND SORRY I COULD NOT TRAVEL BOTH', u'AND BE ONE TRAVELER, LONG I STOOD', u'AND LOOKED DOWN ONE AS FAR AS I COULD', u'TO WHERE IT BENT IN THE UNDERGROWTH;', u'THEN TOOK THE OTHER, AS JUST AS FAIR,', u'AND HAVING PERHAPS THE BETTER CLAIM,', u'BECAUSE IT WAS GRASSY AND WANTED WEAR;', u'THOUGH AS FOR THAT THE PASSING THERE', u'HAD WORN THEM REALLY ABOUT THE SAME,'], [u'AND BOTH THAT MORNING EQUALLY LAY', u'IN LEAVES NO STEP HAD TRODDEN BLACK.', u'OH, I KEPT THE FIRST FOR ANOTHER DAY!', u'YET KNOWING HOW WAY LEADS ON TO WAY,', u'I DOUBTED IF I SHOULD EVER COME BACK.', u'I SHALL BE TELLING THIS WITH A SIGH', u'SOMEWHERE AGES AND AGES HENCE:', u'TWO ROADS DIVERGED IN A WOOD,AND I\u2014', u'I TOOK THE ONE LESS TRAVELED BY,', u'AND THAT HAS MADE ALL THE DIFFERENCE.']]


#filter(function)將rdd2的每個element當作function輸入,只有函數執行結果為True的element,成為rdd3的element
>>> rdd3=rdd2.filter(lambda uline: uline.startswith('I'))

>>> rdd3.collect()
[u'IN LEAVES NO STEP HAD TRODDEN BLACK.', u'I DOUBTED IF I SHOULD EVER COME BACK.', u'I SHALL BE TELLING THIS WITH A SIGH', u'I TOOK THE ONE LESS TRAVELED BY,']

#重新對rdd3進行action時,將利用lineage重新由rdd1->rdd2->rdd3的順序重新執行一次.
>>> rdd3.saveAsTextFile('the_road_not_taken_withI')

[cloudera@cdh6 ~]$ hdfs dfs -ls the_road_not_taken_withI/
Found 3 items
-rw-r--r--   1 cloudera cloudera          0 2019-08-26 19:00 the_road_not_taken_withI/_SUCCESS
-rw-r--r--   1 cloudera cloudera          0 2019-08-26 19:00 the_road_not_taken_withI/part-00000   #partition 1
-rw-r--r--   1 cloudera cloudera        144 2019-08-26 19:00 the_road_not_taken_withI/part-00001   #partition 2

---------------------------------------------------------------------------------------------------------------------------------------------------------
>>> sc
<pyspark.context.SparkContext object at 0x1d8da10>
>>> sc.parallelize([2,3,4])
ParallelCollectionRDD[0] at parallelize at PythonRDD.scala:423
>>> sc.parallelize([2,3,4]).getNumPartitions()
2

>>> sc.parallelize([2,3,4]).collect()
[2, 3, 4]

將父RDD每個element當作function輸入,執行函數的回傳值當作子RDD的element
                            ---
>>> sc.parallelize([2,3,4]).map(lambda x: range(1,x)).collect()
[[1], [1, 2], [1, 2, 3]]
range(1,2),range(1,3),range(1,4)

--自行定義函數
>>> def myfunc1(x):
...     return range(1,x)
...
>>> sc.parallelize([2,3,4]).map(myfunc1).collect()
[[1], [1, 2], [1, 2, 3]]

將父RDD每個element當作function輸入,執行函數的回傳值當作子RDD的多個element
                            -------
>>> sc.parallelize([2,3,4]).flatMap(lambda x: range(1,x)).collect()
[1, 1, 2, 1, 2, 3]

將父RDD每個element當作function輸入,讓函數回傳值為TRUE的element才會當作子RDD的element
                                                      -------
>>> sc.parallelize([2,3,4]).map(lambda x: range(1,x)).filter(lambda x: len(x)>1).collect()
[[1, 2], [1, 2, 3]]

>>> rdd1 = sc.parallelize([1,2,5])
>>> rdd2 = sc.parallelize(range(1,5))
>>> rdd1.collect()
[1, 2, 35]
>>> rdd2.collect()
[1, 2, 3, 4]
>>> rdd1.union(rdd2).collect()
[1, 2, 5, 1, 2, 3, 4]     --結果為其他程式語言的union all
>>> rdd1.union(rdd2).distinct().collect()
[4, 1, 5, 2, 3]           --在union後加上distinct,其結果就等同其他程式語言的union
>>> rdd1.subtract(rdd2).collect()
[5] --出現rdd1但沒有出現在rdd2的rdd1元素
>>> rdd2.subtract(rdd1).collect()
[4, 3]
>>> rdd1.intersection(rdd2).collect()
[1, 2]  --同時出現在rdd1與rdd2元素

>>> rdd1.zip(rdd2).collect()  --錯誤原因是rdd1與rdd2的元素個數不相同
(0 + 1) / 2]19/09/24 22:39:16 WARN scheduler.TaskSetManager: Lost task 0.0 in stage 29.0
(TID 68, cdh6.cloudera, executor 14)
: org.apache.spark.SparkException: Can only zip RDDs with same number of elements in each partition
[Stage 29:=============================>
(1 + 1) / 2]19/09/24 22:39:16 ERROR scheduler.TaskSetManager:
Task 0 in stage 29.0 failed 4 times; aborting job 
--YARN執行一個task時,每次執行稱為一個task attempt(每task至少有一次成功的task attempt)
--當task attempt失敗,AM會在另一個node manager嘗試再一次task attempt,如果成功則完成此task
--如果又再次失敗,AM則會繼續重新嘗試,直到4次task attempt都失敗,則中斷該task

--zip可以用來產生tuple rdd(左邊rdd的元素為key,右邊rdd的元素為value),但兩個rdd的元素個數必須相同
>>> rdd3 = sc.parallelize(range(5,9))
>>> rdd2.zip(rdd3).collect()
[(1, 5), (2, 6), (3, 7), (4, 8)]
>>> rdd3.zip(rdd2).collect()
[(5, 1), (6, 2), (7, 3), (8, 4)]

--元素個數不相同,可以使用cartesian
>>> rdd1.cartesian(rdd2).collect()
[(1, 1), (1, 2), (1, 3), (1, 4), (2, 1), (2, 2), (5, 1), (5, 2), (2, 3), (2, 4), (5, 3), (5, 4)]
>>> rdd1.cartesian(rdd2).count()
12 --rdd1.count()*rdd2.count()

>>> rdd1.getNumPartitions()
2
>>> rdd1.glom().collect()
[[1], [2, 5]]

--mapPartitions(function)將一個partition的所有元素,當作function輸入,每個partition輸出一個結果,當作下一個RDD的元素
>>> rdd1.mapPartitions(lambda x: sum(x)).collect()
TypeError: 'int' object is not iterable --因為mapPartitions所使用的函數必須是可迭代的

>>> def f(iteral):          --iteral代表整個partition的所有elements
...     yield sum(iteral)   --請注意此處yield(所以不能直接使用lambda function)
...
>>> rdd1.mapPartitions(f).collect()
[1, 7]

>>> rdd = sc.parallelize(range(1,10),1)
>>> rdd.collect()
[1, 2, 3, 4, 5, 6, 7, 8, 9]
>>> seqOP=(lambda x,y: (x[0]+y,x[1]+1))
>>> combOP=(lambda x,y: (x[0]+y[0],x[1]+y[1]))
>>> rdd.aggregate((0,0),seqOP,combOP)
(45, 9)

--rddagg是python變數,存在於spark driver
>>> rddagg=rdd.aggregate((0,0),seqOP,combOP)
>>> rddagg
(45, 9)
>>> rddagg[0]/rddagg[1]
5

>>> rdd.sum()
45
>>> rdd.count()
9

--rddsum是python變數
>>> rddsum=rdd.sum()
>>> rddcount=rdd.count()

--此運算發生在spark driver
>>> rddsum/rddcount
5


>>> rdd_tuple.reduce(lambda x,y: x/y)
5

>>> mydata = sc.parallelize(["the cat sat on the mat","the aardvark sat on the sofa"])
>>> mydata.count()
2
>>> mydata2 = mydata.flatMap(lambda line: line.split(' '))
>>> mydata2.count()
12

>>> mydata2.collect()
['the', 'cat', 'sat', 'on', 'the', 'mat', 'the', 'aardvark', 'sat', 'on', 'the', 'sofa']
----------------------------------------  ---------------------------------------------
                                       --------
>>> mydata3 = mydata2.map(lambda word: (word,1))
>>> mydata3.collect()
[('the', 1), ('cat', 1), ('sat', 1), ('on', 1), ('the', 1), ('mat', 1), ('the', 1), ('aardvark', 1), ('sat', 1), ('on', 1), ('the', 1), ('sofa', 1)]

>>> mydata4 = mydata3.reduceByKey(lambda v1,v2: v1+v2)
>>> mydata4.collect()
[('the', 4), ('aardvark', 1), ('on', 2), ('cat', 1), ('mat', 1), ('sofa', 1), ('sat', 2)]
>>> mydata4.saveAsTextFile('word_count')

[cloudera@cdh6 ~]$ hdfs dfs -put the_road_not_taken.txt
[cloudera@cdh6 ~]$ hdfs dfs -ls word_count/
Found 3 items
-rw-r--r--   1 cloudera cloudera          0 2019-09-24 23:56 word_count/_SUCCESS
-rw-r--r--   1 cloudera cloudera         27 2019-09-24 23:56 word_count/part-00000
-rw-r--r--   1 cloudera cloudera         55 2019-09-24 23:56 word_count/part-00001
[cloudera@cdh6 ~]$ hdfs dfs -getmerge word_count word_count.txt
[cloudera@cdh6 ~]$ cat word_count.txt
('the', 4)
('aardvark', 1)
('on', 2)
('cat', 1)
('mat', 1)
('sofa', 1)
('sat', 2)

--以上結果沒有排序
--請使用sortByKey()將結果排序
('aardvark', 1)
('cat', 1)
('mat', 1)
('on', 2)
('sat', 2)
('sofa', 1)
('the', 4)

--再思考一下,如何以值排序
('aardvark',1) -> res
     0      1
sortBy(lambda res: res[1]) --預設為ascending=True
('aardvark', 1)
('cat', 1)
('mat', 1)
('sofa', 1)
('on', 2)
('sat', 2)
('the', 4)

sortBy(lambda res: res[1],False).collect()
('the', 4)
('on', 2)
('sat', 2)
('aardvark', 1)
('cat', 1)
('mat', 1)
('sofa', 1)

http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD
http://spark.apache.org/docs/latest/api/python/_modules/pyspark/rdd.html#RDD

>> sc.parallelize(["the cat sat on the mat","the aardvark sat on the sofa"]).flatMap(lambda line: line.split(' ')).map(lambda word: (word,1)).reduceByKey(lambda v1,v2: v1+v2).map(lambda x: (x[1],x[0])).sortByKey().map(lambda x: (x[1],x[0])).collect()
[('aardvark', 1), ('cat', 1), ('mat', 1), ('sofa', 1), ('on', 2), ('sat', 2), ('the', 4)]

[cloudera@cdh6 ~]$ curl -O http://10.0.1.100:5050/Class_log/apache_logs.txt
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
100 2315k  100 2315k    0     0  32.4M      0 --:--:-- --:--:-- --:--:-- 34.7M

[cloudera@cdh6 ~]$ head apache_logs.txt
83.149.9.216 - - [17/May/2015:10:05:03 +0000] "GET /presentations/logstash-monitorama-2013/images/kibana-search.png HTTP/1.1" 200 203023 "http://semicomplete.com/presentations/logstash-monitorama-2013/" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/32.0.1700.77 Safari/537.36"
83.149.9.216 - - [17/May/2015:10:05:43 +0000] "GET /presentations/logstash-monitorama-2013/images/kibana-dashboard3.png HTTP/1.1" 200 171717 "http://semicomplete.com/presentations/logstash-monitorama-2013/" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/32.0.1700.77 Safari/537.36"
83.149.9.216 - - [17/May/2015:10:05:47 +0000] "GET /presentations/logstash-monitorama-2013/plugin/highlight/highlight.js HTTP/1.1" 200 26185 "http://semicomplete.com/presentations/logstash-monitorama-2013/" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/32.0.1700.77 Safari/537.36"
83.149.9.216 - - [17/May/2015:10:05:12 +0000] "GET /presentations/logstash-monitorama-2013/plugin/zoom-js/zoom.js HTTP/1.1" 200 7697 "http://semicomplete.com/presentations/logstash-monitorama-2013/" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/32.0.1700.77 Safari/537.36"
83.149.9.216 - - [17/May/2015:10:05:07 +0000] "GET /presentations/logstash-monitorama-2013/plugin/notes/notes.js HTTP/1.1" 200 2892 "http://semicomplete.com/presentations/logstash-monitorama-2013/" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/32.0.1700.77 Safari/537.36"
83.149.9.216 - - [17/May/2015:10:05:34 +0000] "GET /presentations/logstash-monitorama-2013/images/sad-medic.png HTTP/1.1" 200 430406 "http://semicomplete.com/presentations/logstash-monitorama-2013/" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/32.0.1700.77 Safari/537.36"
83.149.9.216 - - [17/May/2015:10:05:57 +0000] "GET /presentations/logstash-monitorama-2013/css/fonts/Roboto-Bold.ttf HTTP/1.1" 200 38720 "http://semicomplete.com/presentations/logstash-monitorama-2013/" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/32.0.1700.77 Safari/537.36"
83.149.9.216 - - [17/May/2015:10:05:50 +0000] "GET /presentations/logstash-monitorama-2013/css/fonts/Roboto-Regular.ttf HTTP/1.1" 200 41820 "http://semicomplete.com/presentations/logstash-monitorama-2013/" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/32.0.1700.77 Safari/537.36"
83.149.9.216 - - [17/May/2015:10:05:24 +0000] "GET /presentations/logstash-monitorama-2013/images/frontend-response-codes.png HTTP/1.1" 200 52878 "http://semicomplete.com/presentations/logstash-monitorama-2013/" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/32.0.1700.77 Safari/537.36"
83.149.9.216 - - [17/May/2015:10:05:50 +0000] "GET /presentations/logstash-monitorama-2013/images/kibana-dashboard.png HTTP/1.1" 200 321631 "http://semicomplete.com/presentations/logstash-monitorama-2013/" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/32.0.1700.77 Safari/537.36"

[cloudera@cdh6 ~]$ hdfs dfs -put apache_logs.txt
[cloudera@cdh6 ~]$ pyspark
>>> access_log = sc.textFile('/user/cloudera/apache_logs.txt')
>>> access_log.take(2)
[u'83.149.9.216 - - [17/May/2015:10:05:03 +0000] "GET /presentations/logstash-monitorama-2013/images/kibana-search.png HTTP/1.1" 200 203023 "http://semicomplete.com/presentations/logstash-monitorama-2013/" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/32.0.1700.77 Safari/537.36"', u'83.149.9.216 - - [17/May/2015:10:05:43 +0000] "GET /presentations/logstash-monitorama-2013/images/kibana-dashboard3.png HTTP/1.1" 200 171717 "http://semicomplete.com/presentations/logstash-monitorama-2013/" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/32.0.1700.77 Safari/537.36"']

>>> access_log.map(lambda input: input.split(' ')).take(2)
[[u'83.149.9.216', u'-', u'-', u'[17/May/2015:10:05:03', u'+0000]', u'"GET', u'/presentations/logstash-monitorama-2013/images/kibana-search.png', u'HTTP/1.1"', u'200', u'203023', u'"http://semicomplete.com/presentations/logstash-monitorama-2013/"', u'"Mozilla/5.0', u'(Macintosh;', u'Intel', u'Mac', u'OS', u'X', u'10_9_1)', u'AppleWebKit/537.36', u'(KHTML,', u'like', u'Gecko)', u'Chrome/32.0.1700.77', u'Safari/537.36"'], [u'83.149.9.216', u'-', u'-', u'[17/May/2015:10:05:43', u'+0000]', u'"GET', u'/presentations/logstash-monitorama-2013/images/kibana-dashboard3.png', u'HTTP/1.1"', u'200', u'171717', u'"http://semicomplete.com/presentations/logstash-monitorama-2013/"', u'"Mozilla/5.0', u'(Macintosh;', u'Intel', u'Mac', u'OS', u'X', u'10_9_1)', u'AppleWebKit/537.36', u'(KHTML,', u'like', u'Gecko)', u'Chrome/32.0.1700.77', u'Safari/537.36"']]
       0             1     2                 3               4           5         6                                                                      7        8         9                    10                                                             11              12           13

--找出前五名的來源
>>> access_log.map(lambda input: input.split(' ')).map(lambda x: (x[0],1)).reduceByKey(lambda v1,v2: v1+v2).sortBy(lambda x: x[1],False).take(5)
[(u'66.249.73.135', 482),
 (u'46.105.14.53', 364),
 (u'130.237.218.86', 357),
 (u'75.97.9.59', 273),
 (u'50.16.19.13', 113)]

\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\
--emp.csv的第一行為標頭
>>> empcsv=sc.textFile('emp.csv')
>>> empcsv.collect()
[u'EMPNO,ENAME,JOB,MGR,HIREDATE,SAL,COMM,DEPTNO',           --應該只是標頭,不是資料
 u'7839,KING,PRESIDENT,,17-NOV-81,5000,,10', 
 u'7698,BLAKE,MANAGER,7839,01-MAY-81,2850,,30', 
 u'7782,CLARK,MANAGER,7839,09-JUN-81,2450,,10', 
 u'7566,JONES,MANAGER,7839,02-APR-81,2975,,20', 
 u'7788,SCOTT,ANALYST,7566,19-APR-87,3000,,20', 
 u'7902,FORD,ANALYST,7566,03-DEC-81,3000,,20', 
 u'7369,SMITH,CLERK,7902,17-DEC-80,800,,20', 
 u'7499,ALLEN,SALESMAN,7698,20-FEB-81,1600,300,30', 
 u'7521,WARD,SALESMAN,7698,22-FEB-81,1250,500,30', 
 u'7654,MARTIN,SALESMAN,7698,28-SEP-81,1250,1400,30', 
 u'7844,TURNER,SALESMAN,7698,08-SEP-81,1500,0,30', 
 u'7876,ADAMS,CLERK,7788,23-MAY-87,1100,,20', 
 u'7900,JAMES,CLERK,7698,03-DEC-81,950,,30', 
 u'7934,MILLER,CLERK,7782,23-JAN-82,1300,,10']
 
--移除第一行 
>>> from itertools import islice
>>> empcsv1=empcsv.mapPartitionsWithIndex(lambda indx,iter: islice(iter,1,None) if indx==0 else iter)
>>> empcsv1.collect()
[u'7839,KING,PRESIDENT,,17-NOV-81,5000,,10', 
 u'7698,BLAKE,MANAGER,7839,01-MAY-81,2850,,30', 
 u'7782,CLARK,MANAGER,7839,09-JUN-81,2450,,10', 
 u'7566,JONES,MANAGER,7839,02-APR-81,2975,,20', 
 u'7788,SCOTT,ANALYST,7566,19-APR-87,3000,,20', 
 u'7902,FORD,ANALYST,7566,03-DEC-81,3000,,20', 
 u'7369,SMITH,CLERK,7902,17-DEC-80,800,,20', 
 u'7499,ALLEN,SALESMAN,7698,20-FEB-81,1600,300,30', 
 u'7521,WARD,SALESMAN,7698,22-FEB-81,1250,500,30', 
 u'7654,MARTIN,SALESMAN,7698,28-SEP-81,1250,1400,30', 
 u'7844,TURNER,SALESMAN,7698,08-SEP-81,1500,0,30', 
 u'7876,ADAMS,CLERK,7788,23-MAY-87,1100,,20', 
 u'7900,JAMES,CLERK,7698,03-DEC-81,950,,30', 
 u'7934,MILLER,CLERK,7782,23-JAN-82,1300,,10']

--emp1.csv沒有標頭
[cloudera@cdh6 ~]$ hdfs dfs -cat emp1.csv
7839,KING,PRESIDENT,,17-NOV-81,5000,,10
7698,BLAKE,MANAGER,7839,01-MAY-81,2850,,30
7782,CLARK,MANAGER,7839,09-JUN-81,2450,,10
7566,JONES,MANAGER,7839,02-APR-81,2975,,20
7788,SCOTT,ANALYST,7566,19-APR-87,3000,,20
7902,FORD,ANALYST,7566,03-DEC-81,3000,,20
7369,SMITH,CLERK,7902,17-DEC-80,800,,20
7499,ALLEN,SALESMAN,7698,20-FEB-81,1600,300,30
7521,WARD,SALESMAN,7698,22-FEB-81,1250,500,30
7654,MARTIN,SALESMAN,7698,28-SEP-81,1250,1400,30
7844,TURNER,SALESMAN,7698,08-SEP-81,1500,0,30
7876,ADAMS,CLERK,7788,23-MAY-87,1100,,20
7900,JAMES,CLERK,7698,03-DEC-81,950,,30
7934,MILLER,CLERK,7782,23-JAN-82,1300,,10

>>> empcsv=sc.textFile('emp1.csv')
>>> empcsv.collect()
[u'7839,KING,PRESIDENT,,17-NOV-81,5000,,10', 
 u'7698,BLAKE,MANAGER,7839,01-MAY-81,2850,,30', 
 u'7782,CLARK,MANAGER,7839,09-JUN-81,2450,,10', 
 u'7566,JONES,MANAGER,7839,02-APR-81,2975,,20', 
 u'7788,SCOTT,ANALYST,7566,19-APR-87,3000,,20', 
 u'7902,FORD,ANALYST,7566,03-DEC-81,3000,,20', 
 u'7369,SMITH,CLERK,7902,17-DEC-80,800,,20', 
 u'7499,ALLEN,SALESMAN,7698,20-FEB-81,1600,300,30', 
 u'7521,WARD,SALESMAN,7698,22-FEB-81,1250,500,30', 
 u'7654,MARTIN,SALESMAN,7698,28-SEP-81,1250,1400,30', 
 u'7844,TURNER,SALESMAN,7698,08-SEP-81,1500,0,30', 
 u'7876,ADAMS,CLERK,7788,23-MAY-87,1100,,20', 
 u'7900,JAMES,CLERK,7698,03-DEC-81,950,,30', 
 u'7934,MILLER,CLERK,7782,23-JAN-82,1300,,10']
 
>>> empcsv.count()
14
>>> empcsv.take(1)
[u'7839,KING,PRESIDENT,,17-NOV-81,5000,,10']
>>> emprdd=empcsv.map(lambda line: line.split(','))
>>> emprdd.take(1)
[[u'7839', u'KING', u'PRESIDENT', u'', u'17-NOV-81', u'5000', u'', u'10']]
     0         1           2        3        4           5     6     7
>>> emprdd_pair=emprdd.map(lambda line: (line[7],line[5]))
>>> emprdd_pair.collect()
[(u'10', u'5000'), (u'30', u'2850'), (u'10', u'2450'), (u'20', u'2975'), (u'20', u'3000'), (u'20', u'3000'), (u'20', u'800'), (u'30', u'1600'), (u'30', u'1250'), (u'30', u'1250'), (u'30', u'1500'), (u'20', u'1100'), (u'30', u'950'), (u'10', u'1300')]
>>> emprdd_sum=emprdd_pair.reduceByKey(lambda v1,v2: v1+v2)
>>> emprdd_sum.collect()
[(u'20', u'2975300030008001100'), (u'10', u'500024501300'), (u'30', u'28501600125012501500950')]  --因為sum(字串)的結果為字串連接
                                                 
												 ----將資料型態變更為整數
>>> emprdd_pair=emprdd.map(lambda line: (line[7],int(line[5])))  --請注意數學運算需要數值型態資料
>>> emprdd_pair.collect()
[(u'10', 5000), (u'30', 2850), (u'10', 2450), (u'20', 2975), (u'20', 3000), (u'20', 3000), (u'20', 800), (u'30', 1600), (u'30', 1250), (u'30', 1250), (u'30', 1500), (u'20', 1100), (u'30', 950), (u'10', 1300)]
>>> emprdd_sum=emprdd_pair.reduceByKey(lambda v1,v2: v1+v2)
>>> emprdd_sum.collect()
[(u'20', 10875), (u'10', 8750), (u'30', 9400)]

>>> emprdd_pair1 = emprdd.map(lambda line: (line[7],(int(line[5]),1)))
>>> emprdd_pair1.collect()
[(u'10', (5000, 1)), (u'30', (2850, 1)), (u'10', (2450, 1)), (u'20', (2975, 1)), (u'20', (3000, 1)), (u'20', (3000, 1)), (u'20', (800, 1)), (u'30', (1600, 1)), (u'30', (1250, 1)), (u'30', (1250, 1)), (u'30', (1500, 1)), (u'20', (1100, 1)), (u'30', (950, 1)), (u'10', (1300, 1))]

--reduceByKey(v1,v2) ->當key=10 v1(5000,1),v2(2450,1) -> (7450,2),v2(1300,1) -> (8750,3)
                                                         --------
														     v1
>>> emprdd_avg_tmp = emprdd_pair1.reduceByKey(lambda v1,v2: (v1[0]+v2[0],v1[1]+v2[1]))
>>> emprdd_avg_tmp.collect()
[(u'20', (10875, 5)), (u'10', (8750, 3)), (u'30', (9400, 6))]
  (key,   value)
         (v[0],v[1])
		 -----------
		      v

>>> emprdd_avg = emprdd_avg_tmp.mapValues(lambda v: round(v[0]/v[1]))
>>> emprdd_avg.collect()
[(u'20', 2175.0), (u'10', 2916.0), (u'30', 1566.0)]

>>> emprdd_pair.collect()
[(u'10', 5000), (u'30', 2850), (u'10', 2450), (u'20', 2975), (u'20', 3000), (u'20', 3000), (u'20', 800), (u'30', 1600), (u'30', 1250), (u'30', 1250), (u'30', 1500), (u'20', 1100), (u'30', 950), (u'10', 1300)]
>>> emprdd_pair.aggregateByKey((0,0),lambda a,b: (a[0]+b,a[1]+1),lambda a,b: (a[0]+b[0],a[1]+b[1])).collect()
[(u'20', (10875, 5)), (u'10', (8750, 3)), (u'30', (9400, 6))]
>>> dir(sc)
['PACKAGE_EXTENSIONS', '__class__', '__delattr__', '__dict__', '__doc__', '__enter__', '__exit__', '__format__', '__getattribute__', '__getnewargs__', '__hash__', '__init__', '__module__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__',
 '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_accumulatorServer', '_active_spark_context', '_batchSize', '_callsite', '_checkpointFile', '_conf', '_dictToJavaMap', '_do_init', '_encryption_enabled', '_ensure_initialized', '_gateway', '_getJavaStorageLevel',
 '_initialize_context', '_javaAccumulator', '_jsc', '_jvm', '_lock', '_next_accum_id', '_pickled_broadcast_vars', '_python_includes', '_repr_html_', '_serialize_to_jvm', '_temp_dir', '_unbatched_serializer', 'accumulator', 'addFile', 'addPyFile', 'appName', 'applicationId', 'binaryFiles',
 'binaryRecords', 'broadcast', 'cancelAllJobs', 'cancelJobGroup', 'defaultMinPartitions', 'defaultParallelism', 'dump_profiles', 'emptyRDD', 'environment', 'getConf', 'getLocalProperty', 'getOrCreate', 'hadoopFile', 'hadoopRDD', 'master', 'newAPIHadoopFile', 'newAPIHadoopRDD', 'parallelize',
 'pickleFile', 'profiler_collector', 'pythonExec', 'pythonVer', 'range', 'runJob', 'sequenceFile', 'serializer', 'setCheckpointDir', 'setJobDescription', 'setJobGroup', 'setLocalProperty', 'setLogLevel', 'setSystemProperty', 'show_profiles', 'sparkHome', 'sparkUser', 'startTime', 'statusTracker',
 'stop', 'textFile', 'uiWebUrl', 'union', 'version', 'wholeTextFiles']

>>> sc.version
u'2.4.0.cloudera2'

>>> dir(spark)
['Builder', '__class__', '__delattr__', '__dict__', '__doc__', '__enter__', '__exit__', '__format__', '__getattribute__', '__hash__', '__init__', '__module__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__',
 '__subclasshook__', '__weakref__', '_convert_from_pandas', '_createFromLocal', '_createFromRDD', '_create_from_pandas_with_arrow', '_create_shell_session', '_get_numpy_record_dtype', '_inferSchema', '_inferSchemaFromList', '_instantiatedSession', '_jsc', '_jsparkSession', '_jvm', '_jwrapped',
 '_repr_html_', '_sc', '_wrapped', 'builder', 'catalog', 'conf', 'createDataFrame', 'newSession', 'range', 'read', 'readStream', 'sparkContext', 'sql', 'stop', 'streams', 'table', 'udf', 'version']

>>> mydata4.toDebugString()
'(2) PythonRDD[107] at collect at <stdin>:1 []\n |  MapPartitionsRDD[106] at mapPartitions at PythonRDD.scala:133 []\n |  ShuffledRDD[105] at partitionBy at NativeMethodAccessorImpl.java:0 []\n +-(2) PairwiseRDD[104] at reduceByKey at <stdin>:1 []\n    |  PythonRDD[103] at reduceByKey at <stdin>:1 []\n    |  ParallelCollectionRDD[98] at parallelize at PythonRDD.scala:195 []'

>>> print(mydata4.toDebugString())
(2) PythonRDD[107] at collect at <stdin>:1 []
 |  MapPartitionsRDD[106] at mapPartitions at PythonRDD.scala:133 []
 |  ShuffledRDD[105] at partitionBy at NativeMethodAccessorImpl.java:0 []
 +-(2) PairwiseRDD[104] at reduceByKey at <stdin>:1 []
    |  PythonRDD[103] at reduceByKey at <stdin>:1 []
    |  ParallelCollectionRDD[98] at parallelize at PythonRDD.scala:195 []

\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\
[cloudera@cdh6 ~]$ hdfs dfs -cat /user/cloudera/emp.csv
EMPNO,ENAME,JOB,MGR,HIREDATE,SAL,COMM,DEPTNO
7839,KING,PRESIDENT,,17-NOV-81,5000,,10
7698,BLAKE,MANAGER,7839,01-MAY-81,2850,,30
7782,CLARK,MANAGER,7839,09-JUN-81,2450,,10
7566,JONES,MANAGER,7839,02-APR-81,2975,,20
7788,SCOTT,ANALYST,7566,19-APR-87,3000,,20
7902,FORD,ANALYST,7566,03-DEC-81,3000,,20
7369,SMITH,CLERK,7902,17-DEC-80,800,,20
7499,ALLEN,SALESMAN,7698,20-FEB-81,1600,300,30
7521,WARD,SALESMAN,7698,22-FEB-81,1250,500,30
7654,MARTIN,SALESMAN,7698,28-SEP-81,1250,1400,30
7844,TURNER,SALESMAN,7698,08-SEP-81,1500,0,30
7876,ADAMS,CLERK,7788,23-MAY-87,1100,,20
7900,JAMES,CLERK,7698,03-DEC-81,950,,30
7934,MILLER,CLERK,7782,23-JAN-82,1300,,10

[cloudera@cdh6 ~]$ hdfs dfs -cat /user/cloudera/emp1.csv
7839,KING,PRESIDENT,,17-NOV-81,5000,,10
7698,BLAKE,MANAGER,7839,01-MAY-81,2850,,30
7782,CLARK,MANAGER,7839,09-JUN-81,2450,,10
7566,JONES,MANAGER,7839,02-APR-81,2975,,20
7788,SCOTT,ANALYST,7566,19-APR-87,3000,,20
7902,FORD,ANALYST,7566,03-DEC-81,3000,,20
7369,SMITH,CLERK,7902,17-DEC-80,800,,20
7499,ALLEN,SALESMAN,7698,20-FEB-81,1600,300,30
7521,WARD,SALESMAN,7698,22-FEB-81,1250,500,30
7654,MARTIN,SALESMAN,7698,28-SEP-81,1250,1400,30
7844,TURNER,SALESMAN,7698,08-SEP-81,1500,0,30
7876,ADAMS,CLERK,7788,23-MAY-87,1100,,20
7900,JAMES,CLERK,7698,03-DEC-81,950,,30
7934,MILLER,CLERK,7782,23-JAN-82,1300,,10

--明確指定jdbc驅動程式的位置,讓spark可以直接操作RDBMS或Hive
[cloudera@cdh6 ~]$ pyspark --driver-class-path /usr/share/java/mysql-connector-java.jar --jars /usr/share/java/mysql-connector-java.jar
>>> sc  --core spark程式入口
<SparkContext master=yarn appName=PySparkShell>
>>> spark  --spark sql入口
<pyspark.sql.session.SparkSession object at 0x7fba9b5e6e50>

--請先導入dataframe type與function
>>> from pyspark.sql.types import *             
>>> from pyspark.sql.functions import *

>>> emplist=[('Frank',20000),('Linda',10000),('Wilson',40000)]
>>> type(emplist)
<type 'list'>

>>> emplist
[('Frank', 20000), ('Linda', 10000), ('Wilson', 40000)]

>>> emplistDF=spark.createDataFrame(emplist)
>>> type(emplistDF)
<class 'pyspark.sql.dataframe.DataFrame'>

>>> emplistDF.collect()  --rdd.collect(),dataframe由row object組成
[Row(_1=u'Frank', _2=20000), Row(_1=u'Linda', _2=10000), Row(_1=u'Wilson', _2=40000)]

>>> emplistDF.show()
+------+-----+
|    _1|   _2|
+------+-----+
| Frank|20000|
| Linda|10000|
|Wilson|40000|
+------+-----+

>>> emplistDF.printSchema()
root
 |-- _1: string (nullable = true)  --使用推導方式產生
 |-- _2: long (nullable = true)

>>> dir(emplistDF)
['__class__', '__delattr__', '__dict__', '__doc__', '__format__', '__getattr__', 
 '__getattribute__', '__getitem__', '__hash__', '__init__', '__module__', '__new__',
 '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__',
 '__subclasshook__', '__weakref__', '_collectAsArrow', '_jcols', '_jdf', '_jmap',
 '_jseq', '_lazy_rdd', '_repr_html_', '_sc', '_schema', '_sort_cols', '_support_repr_html',
 'agg', 'alias', 'approxQuantile', 'cache', 'checkpoint', 'coalesce', 'colRegex', 'collect',
 'columns', 'corr', 'count', 'cov', 'createGlobalTempView', 'createOrReplaceGlobalTempView',
 'createOrReplaceTempView', 'createTempView', 'crossJoin', 'crosstab', 'cube', 'describe',
 'distinct', 'drop', 'dropDuplicates', 'drop_duplicates', 'dropna', 'dtypes', 'exceptAll',
 'explain', 'fillna', 'filter', 'first', 'foreach', 'foreachPartition', 'freqItems', 'groupBy',
 'groupby', 'head', 'hint', 'intersect', 'intersectAll', 'isLocal', 'isStreaming', 'is_cached',
 'join', 'limit', 'localCheckpoint', 'na', 'orderBy', 'persist', 'printSchema', 'randomSplit',
 'rdd', 'registerTempTable', 'repartition', 'repartitionByRange', 'replace', 'rollup',
 'sample', 'sampleBy', 'schema', 'select', 'selectExpr', 'show', 'sort', 'sortWithinPartitions',
 'sql_ctx', 'stat', 'storageLevel', 'subtract', 'summary', 'take', 'toDF', 'toJSON', 'toLocalIterator',
 'toPandas', 'union', 'unionAll', 'unionByName', 'unpersist', 'where', 'withColumn', 'withColumnRenamed',
 'withWatermark', 'write', 'writeStream']

>>> help(spark.read.csv)
Help on method csv in module pyspark.sql.readwriter:

csv(self, path, schema=None, sep=None, encoding=None, quote=None, escape=None, comment=None, header=None, inferSchema=None, ignoreLeadingWhiteSpace=None,
    ignoreTrailingWhiteSpace=None, nullValue=None, nanValue=None, positiveInf=None, negativeInf=None, dateFormat=None, timestampFormat=None, maxColumns=None,
	maxCharsPerColumn=None, maxMalformedLogPerPartition=None, mode=None, columnNameOfCorruptRecord=None, multiLine=None, charToEscapeQuoteEscaping=None,
	samplingRatio=None, enforceSchema=None, emptyValue=None) method of pyspark.sql.readwriter.DataFrameReader instance

--option('header','true')表示emp.csv的第一行是此DataFrame的欄位名字
>>> empDF=spark.read.format('csv').option('header','true').load('emp.csv')
--另兩種寫法
--empDF = spark.read.load("/user/cloudera/emp.csv",format="csv", sep=",", inferSchema="true", header="true")
--empDF = spark.read.csv('/user/cloudera/emp.csv',header='true',inferSchema='true')

>>> empDF.printSchema()
root
 |-- EMPNO: string (nullable = true)      --因為沒有明確指定綱要,所以目前使用推論方式來判斷資料結構
 |-- ENAME: string (nullable = true)      --inferSchema="false"
 |-- JOB: string (nullable = true)
 |-- MGR: string (nullable = true)
 |-- HIREDATE: string (nullable = true)
 |-- SAL: string (nullable = true)        --string type不能進行數學運算
 |-- COMM: string (nullable = true)
 |-- DEPTNO: string (nullable = true)

--如果inferSchema="true",將自動推斷欄位的資料型態
root
 |-- EMPNO: integer (nullable = true)
 |-- ENAME: string (nullable = true)
 |-- JOB: string (nullable = true)
 |-- MGR: integer (nullable = true)
 |-- HIREDATE: string (nullable = true)
 |-- SAL: integer (nullable = true)       --資料型態變成整數,將可以進行數學運算
 |-- COMM: integer (nullable = true)
 |-- DEPTNO: integer (nullable = true)

>>> empDF.show
<bound method DataFrame.show of DataFrame[EMPNO: string, ENAME: string, JOB: string, MGR: string, HIREDATE: string, SAL: string, COMM: string, DEPTNO: string]>

--請注意即便函數沒有參數可供輸入,在spark呼叫函數時,()是不可免除的
>>> empDF.show()
+-----+------+---------+----+---------+----+----+------+
|EMPNO| ENAME|      JOB| MGR| HIREDATE| SAL|COMM|DEPTNO|
+-----+------+---------+----+---------+----+----+------+
| 7839|  KING|PRESIDENT|null|17-NOV-81|5000|null|    10|
| 7698| BLAKE|  MANAGER|7839|01-MAY-81|2850|null|    30|
| 7782| CLARK|  MANAGER|7839|09-JUN-81|2450|null|    10|
| 7566| JONES|  MANAGER|7839|02-APR-81|2975|null|    20|
| 7788| SCOTT|  ANALYST|7566|19-APR-87|3000|null|    20|
| 7902|  FORD|  ANALYST|7566|03-DEC-81|3000|null|    20|
| 7369| SMITH|    CLERK|7902|17-DEC-80| 800|null|    20|
| 7499| ALLEN| SALESMAN|7698|20-FEB-81|1600| 300|    30|
| 7521|  WARD| SALESMAN|7698|22-FEB-81|1250| 500|    30|
| 7654|MARTIN| SALESMAN|7698|28-SEP-81|1250|1400|    30|
| 7844|TURNER| SALESMAN|7698|08-SEP-81|1500|   0|    30|
| 7876| ADAMS|    CLERK|7788|23-MAY-87|1100|null|    20|
| 7900| JAMES|    CLERK|7698|03-DEC-81| 950|null|    30|
| 7934|MILLER|    CLERK|7782|23-JAN-82|1300|null|    10|
+-----+------+---------+----+---------+----+----+------+

>>> empDF.head()  #head()取回empDF的第一筆row(類似rdd.take(1))
--請試看看empDF.show(1)
Row(EMPNO=u'7839', ENAME=u'KING', JOB=u'PRESIDENT', MGR=None, HIREDATE=u'17-NOV-81', SAL=u'5000', COMM=None, DEPTNO=u'10')
>>> empDF.head(2) #head(2)取回empDF的前2筆row(類似rdd.take(2))
--請試看看empDF.show(2)
[Row(EMPNO=u'7839', ENAME=u'KING', JOB=u'PRESIDENT', MGR=None, HIREDATE=u'17-NOV-81', SAL=u'5000', COMM=None, DEPTNO=u'10'), Row(EMPNO=u'7698', ENAME=u'BLAKE', JOB=u'MANAGER', MGR=u'7839', HIREDATE=u'01-MAY-81', SAL=u'2850', COMM=None, DEPTNO=u'30')]

>>> empDF.show(5)
+-----+-----+---------+----+---------+----+----+------+
|EMPNO|ENAME|      JOB| MGR| HIREDATE| SAL|COMM|DEPTNO|
+-----+-----+---------+----+---------+----+----+------+
| 7839| KING|PRESIDENT|null|17-NOV-81|5000|null|    10|
| 7698|BLAKE|  MANAGER|7839|01-MAY-81|2850|null|    30|
| 7782|CLARK|  MANAGER|7839|09-JUN-81|2450|null|    10|
| 7566|JONES|  MANAGER|7839|02-APR-81|2975|null|    20|
| 7788|SCOTT|  ANALYST|7566|19-APR-87|3000|null|    20|
+-----+-----+---------+----+---------+----+----+------+
only showing top 5 rows

>>> empDF.head(5)
[Row(EMPNO=u'7839', ENAME=u'KING', JOB=u'PRESIDENT', MGR=None, HIREDATE=u'17-NOV-81', SAL=u'5000', COMM=None, DEPTNO=u'10'), Row(EMPNO=u'7698', ENAME=u'BLAKE', JOB=u'MANAGER', MGR=u'7839', HIREDATE=u'01-MAY-81', SAL=u'2850', COMM=None, DEPTNO=u'30'), Row(EMPNO=u'7782', ENAME=u'CLARK', JOB=u'MANAGER', MGR=u'7839', HIREDATE=u'09-JUN-81', SAL=u'2450', COMM=None, DEPTNO=u'10'), Row(EMPNO=u'7566', ENAME=u'JONES', JOB=u'MANAGER', MGR=u'7839', HIREDATE=u'02-APR-81', SAL=u'2975', COMM=None, DEPTNO=u'20'), Row(EMPNO=u'7788', ENAME=u'SCOTT', JOB=u'ANALYST', MGR=u'7566', HIREDATE=u'19-APR-87', SAL=u'3000', COMM=None, DEPTNO=u'20')]


--請注意,執行此指令前,應該先執行from pyspark.sql.types import *
>>> from pyspark.sql.types import *
                   每個欄位使用一個StructField()宣告
>>> empcollists = [StructField('empno',IntegerType(),False),StructField('name',StringType(),True),StructField('job',StringType(),True),StructField('mgr',IntegerType(),True),StructField('hiredate',StringType(),True),StructField('sal',IntegerType(),True),StructField('comm',IntegerType(),True),StructField('deptno',IntegerType(),True)]
                               ------- ------------- ------
							   欄位名稱 資料型態     可否為空值
>>> type(empcollists)
<type 'list'>

>>> empschema = StructType(empcollists)
>>> type(empschema)
<class 'pyspark.sql.types.StructType'>

--請注意,所讀入的檔案為emp1.csv(沒有標題行)                ---------------- --------------
>>> empDF = empDF=spark.read.csv('/user/cloudera/emp1.csv',schema=empschema,header='false')
>>> empDF.printSchema()
root
 |-- empno: integer (nullable = true)
 |-- name: string (nullable = true)
 |-- job: string (nullable = true)
 |-- mgr: integer (nullable = true)
 |-- hiredate: string (nullable = true)
 |-- sal: integer (nullable = true)
 |-- comm: integer (nullable = true)
 |-- deptno: integer (nullable = true)
 
>>> emplistDF.printSchema()
root
 |-- _1: string (nullable = true)
 |-- _2: long (nullable = true)

>>> empcollists=[StructField('name',StringType(),True),StructField('salary',IntegerType(),True)]
>>> emplistschema=StructType(empcollists)
>>> emplistschema
StructType(List(StructField(name,StringType,true),StructField(salary,IntegerType,true)))
>>> emplist
[('Frank', 20000), ('Linda', 10000), ('Wilson', 40000)]
>>> emplistDF=spark.createDataFrame(emplist,emplistschema)
>>> emplistDF.show()
+------+------+
|  name|salary|
+------+------+
| Frank| 20000|
| Linda| 10000|
|Wilson| 40000|
+------+------+

>>> emplistDF.printSchema()
root
 |-- name: string (nullable = true)
 |-- salary: integer (nullable = true)

>>> empcollists = [StructField('empno',StringType(),False),StructField('name',StringType(),True),StructField('job',StringType(),True),StructField('mgr',IntegerType(),True),StructField('hiredate',StringType(),True),StructField('sal',IntegerType(),True),StructField('comm',IntegerType(),True),StructField('deptno',StringType(),True)]
>>> empschema = StructType(empcollists)
>>> empDF = spark.read.format('csv').option('header','false').schema(empschema).load('emp1.csv')  #option('header','false')表示emp1.csv的第一行不是欄位名字,而是一般欄位值
>>> empDF.show()
+-----+------+---------+----+---------+----+----+------+
|empno|  name|      job| mgr| hiredate| sal|comm|deptno|
+-----+------+---------+----+---------+----+----+------+
| 7839|  KING|PRESIDENT|null|17-NOV-81|5000|null|    10|
| 7698| BLAKE|  MANAGER|7839|01-MAY-81|2850|null|    30|
| 7782| CLARK|  MANAGER|7839|09-JUN-81|2450|null|    10|
| 7566| JONES|  MANAGER|7839|02-APR-81|2975|null|    20|
| 7788| SCOTT|  ANALYST|7566|19-APR-87|3000|null|    20|
| 7902|  FORD|  ANALYST|7566|03-DEC-81|3000|null|    20|
| 7369| SMITH|    CLERK|7902|17-DEC-80| 800|null|    20|
| 7499| ALLEN| SALESMAN|7698|20-FEB-81|1600| 300|    30|
| 7521|  WARD| SALESMAN|7698|22-FEB-81|1250| 500|    30|
| 7654|MARTIN| SALESMAN|7698|28-SEP-81|1250|1400|    30|
| 7844|TURNER| SALESMAN|7698|08-SEP-81|1500|   0|    30|
| 7876| ADAMS|    CLERK|7788|23-MAY-87|1100|null|    20|
| 7900| JAMES|    CLERK|7698|03-DEC-81| 950|null|    30|
| 7934|MILLER|    CLERK|7782|23-JAN-82|1300|null|    10|
+-----+------+---------+----+---------+----+----+------+

>>> empDF.printSchema()
root
 |-- empno: string (nullable = true)
 |-- name: string (nullable = true)
 |-- job: string (nullable = true)
 |-- mgr: integer (nullable = true)
 |-- hiredate: string (nullable = true)
 |-- sal: integer (nullable = true)
 |-- comm: integer (nullable = true)
 |-- deptno: string (nullable = true)
 
--如果資料來源為emp.csv(第一行為標題行)

\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\
[cloudera@cdh6 ~]$ mysql -uroot -pcloudera
Welcome to the MySQL monitor.  Commands end with ; or \g.
Your MySQL connection id is 2570
Server version: 5.1.73 Source distribution

Copyright (c) 2000, 2013, Oracle and/or its affiliates. All rights reserved.

Oracle is a registered trademark of Oracle Corporation and/or its
affiliates. Other names may be trademarks of their respective
owners.

Type 'help;' or '\h' for help. Type '\c' to clear the current input statement.

mysql> use scott
Reading table information for completion of table and column names
You can turn off this feature to get a quicker startup with -A

Database changed
mysql> show tables;
+-----------------+
| Tables_in_scott |
+-----------------+
| bonus           |
| dept            |
| emp             |
| salgrade        |
+-----------------+
4 rows in set (0.00 sec)

mysql> select * from emp;
+-------+--------+-----------+------+------------+---------+---------+--------+
| empno | ename  | job       | mgr  | hiredate   | sal     | comm    | deptno |
+-------+--------+-----------+------+------------+---------+---------+--------+
|  7369 | SMITH  | CLERK     | 7902 | 1980-12-17 |  800.00 |    NULL |     20 |
|  7499 | ALLEN  | SALESMAN  | 7698 | 1981-02-20 | 1600.00 |  300.00 |     30 |
|  7521 | WARD   | SALESMAN  | 7698 | 1981-02-22 | 1250.00 |  500.00 |     30 |
|  7566 | JONES  | MANAGER   | 7839 | 1981-04-02 | 2975.00 |    NULL |     20 |
|  7654 | MARTIN | SALESMAN  | 7698 | 1981-09-28 | 1250.00 | 1400.00 |     30 |
|  7698 | BLAKE  | MANAGER   | 7839 | 1981-05-01 | 2850.00 |    NULL |     30 |
|  7782 | CLARK  | MANAGER   | 7839 | 1981-06-09 | 2450.00 |    NULL |     10 |
|  7788 | SCOTT  | ANALYST   | 7566 | 1982-12-09 | 3000.00 |    NULL |     20 |
|  7839 | KING   | PRESIDENT | NULL | 1981-11-17 | 5000.00 |    NULL |     10 |
|  7844 | TURNER | SALESMAN  | 7698 | 1981-09-08 | 1500.00 |    0.00 |     30 |
|  7876 | ADAMS  | CLERK     | 7788 | 1983-01-12 | 1100.00 |    NULL |     20 |
|  7900 | JAMES  | CLERK     | 7698 | 1981-12-03 |  950.00 |    NULL |     30 |
|  7902 | FORD   | ANALYST   | 7566 | 1981-12-03 | 3000.00 |    NULL |     20 |
|  7934 | MILLER | CLERK     | 7782 | 1982-01-23 | 1300.00 |    NULL |     10 |
+-------+--------+-----------+------+------------+---------+---------+--------+
14 rows in set (0.01 sec)

mysql> desc emp;
+----------+--------------+------+-----+---------+-------+
| Field    | Type         | Null | Key | Default | Extra |
+----------+--------------+------+-----+---------+-------+
| empno    | decimal(4,0) | NO   |     | NULL    |       |
| ename    | varchar(10)  | YES  |     | NULL    |       |
| job      | varchar(9)   | YES  |     | NULL    |       |
| mgr      | decimal(4,0) | YES  |     | NULL    |       |
| hiredate | date         | YES  |     | NULL    |       |
| sal      | decimal(7,2) | YES  |     | NULL    |       |
| comm     | decimal(7,2) | YES  |     | NULL    |       |
| deptno   | decimal(2,0) | YES  |     | NULL    |       |
+----------+--------------+------+-----+---------+-------+
8 rows in set (0.00 sec)
\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\
 --資料來源為MySQL Database
>>> dfEmp=spark.read.jdbc('jdbc:mysql://cdh6/scott','emp',properties={'user':'root','password':'cloudera'})
                                                  -----
												  database name
======================================================================================================================
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "/opt/cloudera/parcels/SPARK2-2.4.0.cloudera2-1.cdh5.13.3.p0.1041012/lib/spark2/python/pyspark/sql/readwriter.py", line 556, in jdbc
    return self._df(self._jreader.jdbc(url, table, jprop))
  File "/opt/cloudera/parcels/SPARK2-2.4.0.cloudera2-1.cdh5.13.3.p0.1041012/lib/spark2/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py", line 1257, in __call__
  File "/opt/cloudera/parcels/SPARK2-2.4.0.cloudera2-1.cdh5.13.3.p0.1041012/lib/spark2/python/pyspark/sql/utils.py", line 63, in deco
    return f(*a, **kw)
  File "/opt/cloudera/parcels/SPARK2-2.4.0.cloudera2-1.cdh5.13.3.p0.1041012/lib/spark2/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py", line 328, in get_return_value
py4j.protocol.Py4JJavaError: An error occurred while calling o142.jdbc.
: java.sql.SQLException: No suitable driver --啟動pyspark時,沒有明確指定所使用的jdbc driver jar 
========================================================================================================================
>>> dfEmp.printSchema()
root
 |-- empno: decimal(4,0) (nullable = true)
 |-- ename: string (nullable = true)
 |-- job: string (nullable = true)
 |-- mgr: decimal(4,0) (nullable = true)
 |-- hiredate: date (nullable = true)
 |-- sal: decimal(7,2) (nullable = true)
 |-- comm: decimal(7,2) (nullable = true)
 |-- deptno: decimal(2,0) (nullable = true)  --直接使用mysql表格的綱要

>>> empDF.printSchema()
root
 |-- empno: string (nullable = true)
 |-- name: string (nullable = true)
 |-- job: string (nullable = true)
 |-- mgr: integer (nullable = true)
 |-- hiredate: string (nullable = true)
 |-- sal: integer (nullable = true)
 |-- comm: integer (nullable = true)
 |-- deptno: string (nullable = true)

>>> empDF.dtypes
[('empno', 'int'), ('name', 'string'), ('job', 'string'), ('mgr', 'int'), ('hiredate', 'string'), ('sal', 'int'), ('comm', 'int'), ('deptno', 'int')]
\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\
[cloudera@cdh6 ~]$ curl -O http://10.0.1.100:5050/Class_log/people.json
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
  0    78    0    78    0     0   6126      0 --:--:-- --:--:-- --:--:--  8666
[cloudera@cdh6 ~]$ hdfs dfs -put people.json
[cloudera@cdh6 ~]$ hdfs dfs -ls /user/cloudera/people.json
-rw-r--r--   1 cloudera cloudera         77 2020-03-28 20:28 /user/cloudera/people.json
[cloudera@cdh6 ~]$ hdfs dfs -cat /user/cloudera/people.json
{"name":"Frank"}
{"name":"Linda","age":40}
{"name":"Wilson","city":"Taiepi"}
\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\
>>> peopleDF=spark.read.json('/user/cloudera/people.json')
>>> peopleDF.printSchema()
root
 |-- age: long (nullable = true)
 |-- city: string (nullable = true)
 |-- name: string (nullable = true)

>>> peopleDF.show()
+----+------+------+
| age|  city|  name|
+----+------+------+
|null|  null| Frank|
|  40|  null| Linda|
|null|Taiepi|Wilson|
+----+------+------+

>>> peopleDF.dtypes
[('age', 'bigint'), ('city', 'string'), ('name', 'string')]

\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\
[cloudera@cdh6 ~]$ which parquet-tools
/usr/bin/parquet-tools

[cloudera@cdh6 ~]$ parquet-tools schema emp.parquet
message spark_schema {
  optional int32 empno;
  optional binary name (UTF8);
  optional binary job (UTF8);
  optional int32 mgr;
  optional binary hiredate (UTF8);
  optional int32 sal;
  optional int32 comm;
  optional int32 deptno;
}

[cloudera@cdh6 ~]$ parquet-tools cat --json emp.parquet
{"empno":7839,"name":"KING","job":"PRESIDENT","hiredate":"17-NOV-81","sal":5000,"deptno":10}
{"empno":7698,"name":"BLAKE","job":"MANAGER","mgr":7839,"hiredate":"01-MAY-81","sal":2850,"deptno":30}
{"empno":7782,"name":"CLARK","job":"MANAGER","mgr":7839,"hiredate":"09-JUN-81","sal":2450,"deptno":10}
{"empno":7566,"name":"JONES","job":"MANAGER","mgr":7839,"hiredate":"02-APR-81","sal":2975,"deptno":20}
{"empno":7788,"name":"SCOTT","job":"ANALYST","mgr":7566,"hiredate":"19-APR-87","sal":3000,"deptno":20}
{"empno":7902,"name":"FORD","job":"ANALYST","mgr":7566,"hiredate":"03-DEC-81","sal":3000,"deptno":20}
{"empno":7369,"name":"SMITH","job":"CLERK","mgr":7902,"hiredate":"17-DEC-80","sal":800,"deptno":20}
{"empno":7499,"name":"ALLEN","job":"SALESMAN","mgr":7698,"hiredate":"20-FEB-81","sal":1600,"comm":300,"deptno":30}
{"empno":7521,"name":"WARD","job":"SALESMAN","mgr":7698,"hiredate":"22-FEB-81","sal":1250,"comm":500,"deptno":30}
{"empno":7654,"name":"MARTIN","job":"SALESMAN","mgr":7698,"hiredate":"28-SEP-81","sal":1250,"comm":1400,"deptno":30}
{"empno":7844,"name":"TURNER","job":"SALESMAN","mgr":7698,"hiredate":"08-SEP-81","sal":1500,"comm":0,"deptno":30}
{"empno":7876,"name":"ADAMS","job":"CLERK","mgr":7788,"hiredate":"23-MAY-87","sal":1100,"deptno":20}
{"empno":7900,"name":"JAMES","job":"CLERK","mgr":7698,"hiredate":"03-DEC-81","sal":950,"deptno":30}
{"empno":7934,"name":"MILLER","job":"CLERK","mgr":7782,"hiredate":"23-JAN-82","sal":1300,"deptno":10}

[cloudera@cdh6 ~]$ parquet-tools head -n 5 emp.parquet
empno = 7839
name = KING
job = PRESIDENT
hiredate = 17-NOV-81
sal = 5000
deptno = 10

empno = 7698
name = BLAKE
job = MANAGER
mgr = 7839
hiredate = 01-MAY-81
sal = 2850
deptno = 30

empno = 7782
name = CLARK
job = MANAGER
mgr = 7839
hiredate = 09-JUN-81
sal = 2450
deptno = 10

empno = 7566
name = JONES
job = MANAGER
mgr = 7839
hiredate = 02-APR-81
sal = 2975
deptno = 20

empno = 7788
name = SCOTT
job = ANALYST
mgr = 7566
hiredate = 19-APR-87
sal = 3000
deptno = 20

[cloudera@cdh6 ~]$ parquet-tools meta emp.parquet
creator:     parquet-mr version 1.5.0-cdh5.13.3 (build ${buildNumber})
extra:       org.apache.spark.sql.parquet.row.metadata = {"type":"struct","fiel [more]...

file schema: spark_schema
-----------------------------------------------------------------------------------------
empno:       OPTIONAL INT32 R:0 D:1
name:        OPTIONAL BINARY O:UTF8 R:0 D:1
job:         OPTIONAL BINARY O:UTF8 R:0 D:1
mgr:         OPTIONAL INT32 R:0 D:1
hiredate:    OPTIONAL BINARY O:UTF8 R:0 D:1
sal:         OPTIONAL INT32 R:0 D:1
comm:        OPTIONAL INT32 R:0 D:1
deptno:      OPTIONAL INT32 R:0 D:1

row group 1: RC:14 TS:936
-----------------------------------------------------------------------------------------
empno:        INT32 SNAPPY DO:0 FPO:4 SZ:99/96/0.97 VC:14 ENC:PLAIN,RLE,BIT_PACKED
name:         BINARY SNAPPY DO:0 FPO:103 SZ:159/168/1.06 VC:14 ENC:PLAIN,RLE,BIT_PACKED
job:          BINARY SNAPPY DO:0 FPO:262 SZ:126/123/0.98 VC:14 ENC:RLE,BIT_PACK [more]...
mgr:          INT32 SNAPPY DO:0 FPO:388 SZ:89/85/0.96 VC:14 ENC:RLE,BIT_PACKED, [more]...
hiredate:     BINARY SNAPPY DO:0 FPO:477 SZ:192/243/1.27 VC:14 ENC:RLE,BIT_PACK [more]...
sal:          INT32 SNAPPY DO:0 FPO:669 SZ:90/95/1.06 VC:14 ENC:PLAIN,RLE,BIT_PACKED
comm:         INT32 SNAPPY DO:0 FPO:759 SZ:58/56/0.97 VC:14 ENC:PLAIN,RLE,BIT_PACKED
deptno:       INT32 SNAPPY DO:0 FPO:817 SZ:74/70/0.95 VC:14 ENC:RLE,BIT_PACKED, [more]...

[cloudera@cdh6 ~]$ hdfs dfs -put emp.parquet
[cloudera@cdh6 ~]$ hdfs dfs -ls emp.parquet
-rw-r--r--   1 cloudera cloudera       2082 2020-03-28 22:34 emp.parquet

[cloudera@cdh6 ~]$ pyspark
>>> emp=spark.read.parquet('/user/cloudera/emp.parquet')
>>> emp.printSchema()                
root 
 |-- empno: integer (nullable = true)      --不需要特別指定綱要,因為parquet file內崁綱要
 |-- name: string (nullable = true)
 |-- job: string (nullable = true)
 |-- mgr: integer (nullable = true)
 |-- hiredate: string (nullable = true)
 |-- sal: integer (nullable = true)
 |-- comm: integer (nullable = true)
 |-- deptno: integer (nullable = true)

>>> emp.dtypes
[('empno', 'int'), ('name', 'string'), ('job', 'string'), ('mgr', 'int'), ('hiredate', 'string'), ('sal', 'int'), ('comm', 'int'), ('deptno', 'int')]
>>> emp.columns
['empno', 'name', 'job', 'mgr', 'hiredate', 'sal', 'comm', 'deptno']

--Spark讀取Hive資料庫內容(其實是連線到Hive metastore service)
>>> spark.sql('show databases').show()
+------------+
|databaseName|
+------------+
|     default|
+------------+

>>> spark.sql('show tables').show()
+--------+---------+-----------+
|database|tableName|isTemporary|
+--------+---------+-----------+
| default|customers|      false|   --在Module01課程中,使用sqoop所導入的hive table
| default|sample_07|      false|
| default|sample_08|      false|
| default| web_logs|      false|
+--------+---------+-----------+

>>> spark.sql('desc customers').show()
+-----------------+---------+-------+
|         col_name|data_type|comment|
+-----------------+---------+-------+
|      customer_id|      int|   null|
|   customer_fname|   string|   null|
|   customer_lname|   string|   null|
|   customer_email|   string|   null|
|customer_password|   string|   null|
|  customer_street|   string|   null|
|    customer_city|   string|   null|
|   customer_state|   string|   null|
| customer_zipcode|   string|   null|
+-----------------+---------+-------+

>>> spark.sql('desc formatted customers').show()
+--------------------+--------------------+-------+
|            col_name|           data_type|comment|
+--------------------+--------------------+-------+
|         customer_id|                 int|   null|
|      customer_fname|              string|   null|
|      customer_lname|              string|   null|
|      customer_email|              string|   null|
|   customer_password|              string|   null|
|     customer_street|              string|   null|
|       customer_city|              string|   null|
|      customer_state|              string|   null|
|    customer_zipcode|              string|   null|
|                    |                    |       |
|# Detailed Table ...|                    |       |
|            Database|             default|       |
|               Table|           customers|       |
|               Owner|            cloudera|       |
|        Created Time|Tue Sep 15 10:28:...|       |
|         Last Access|Thu Jan 01 08:00:...|       |
|          Created By|  Spark 2.2 or prior|       |
|                Type|             MANAGED|       |
|            Provider|                hive|       |
|             Comment|Imported by sqoop...|       |
+--------------------+--------------------+-------+
only showing top 20 rows

>>> spark.sql('desc formatted customers').show(100)
+--------------------+--------------------+-------+
|            col_name|           data_type|comment|
+--------------------+--------------------+-------+
|         customer_id|                 int|   null|
|      customer_fname|              string|   null|
|      customer_lname|              string|   null|
|      customer_email|              string|   null|
|   customer_password|              string|   null|
|     customer_street|              string|   null|
|       customer_city|              string|   null|
|      customer_state|              string|   null|
|    customer_zipcode|              string|   null|
|                    |                    |       |
|# Detailed Table ...|                    |       |
|            Database|             default|       |
|               Table|           customers|       |
|               Owner|            cloudera|       |
|        Created Time|Tue Sep 15 10:28:...|       |
|         Last Access|Thu Jan 01 08:00:...|       |
|          Created By|  Spark 2.2 or prior|       |
|                Type|             MANAGED|       |
|            Provider|                hive|       |
|             Comment|Imported by sqoop...|       |
|    Table Properties|[numFilesErasureC...|       |
|          Statistics|        953525 bytes|       |
|            Location|hdfs://cdh6:8020/...|       |
|       Serde Library|org.apache.hadoop...|       |
|         InputFormat|org.apache.hadoop...|       |
|        OutputFormat|org.apache.hadoop...|       |
|  Storage Properties|[serialization.fo...|       |
|  Partition Provider|             Catalog|       |
+--------------------+--------------------+-------+

--從Hive Table取得資料,並且接受Hive Table定義
>>> custsDF=spark.read.table('customers')    
>>> custsDF.printSchema()
root
 |-- customer_id: integer (nullable = true)
 |-- customer_fname: string (nullable = true)
 |-- customer_lname: string (nullable = true)
 |-- customer_email: string (nullable = true)
 |-- customer_password: string (nullable = true)
 |-- customer_street: string (nullable = true)
 |-- customer_city: string (nullable = true)
 |-- customer_state: string (nullable = true)
 |-- customer_zipcode: string (nullable = true)

>>> custsDF.show(5)
+-----------+--------------+--------------+--------------+-----------------+--------------------+-------------+--------------+----------------+
|customer_id|customer_fname|customer_lname|customer_email|customer_password|     customer_street|customer_city|customer_state|customer_zipcode|
+-----------+--------------+--------------+--------------+-----------------+--------------------+-------------+--------------+----------------+
|          1|       Richard|     Hernandez|     XXXXXXXXX|        XXXXXXXXX|  6303 Heather Plaza|  Brownsville|            TX|           78521|
|          2|          Mary|       Barrett|     XXXXXXXXX|        XXXXXXXXX|9526 Noble Embers...|    Littleton|            CO|           80126|
|          3|           Ann|         Smith|     XXXXXXXXX|        XXXXXXXXX|3422 Blue Pioneer...|       Caguas|            PR|           00725|
|          4|          Mary|         Jones|     XXXXXXXXX|        XXXXXXXXX|  8324 Little Common|   San Marcos|            CA|           92069|
|          5|        Robert|        Hudson|     XXXXXXXXX|        XXXXXXXXX|10 Crystal River ...|       Caguas|            PR|           00725|
+-----------+--------------+--------------+--------------+-----------------+--------------------+-------------+--------------+----------------+
only showing top 5 rows

>>> custs_count_by_city_DF=spark.sql('select customer_city,count(*) count_by_city from customers group by customer_city')
>>> custs_count_by_city_DF.show()
+---------------+-------------+
|  customer_city|count_by_city|
+---------------+-------------+
|        Hanover|            9|
|         Caguas|         4584|
|         Corona|           25|
|          Tempe|           35|
|  Bowling Green|            8|
|    Springfield|            3|
|  Lawrenceville|           12|
|North Las Vegas|           12|
|       Palatine|            8|
|        Phoenix|           64|
|     Plainfield|           13|
|      Bountiful|            8|
|      Hollywood|           24|
|       Waukegan|            9|
|      Pittsburg|            4|
|      Levittown|            8|
| Mount Prospect|            7|
|       Toa Alta|            3|
|       Brighton|           13|
|     Toms River|           11|
+---------------+-------------+
only showing top 20 rows

>>> spark.sql('select customer_city,count(*) count_by_city from customers group by customer_city order by count_by_city DESC').show()
+--------------+-------------+
| customer_city|count_by_city|
+--------------+-------------+
|        Caguas|         4584|
|       Chicago|          274|
|      Brooklyn|          225|
|   Los Angeles|          224|
|      New York|          120|
|  Philadelphia|          105|
|         Bronx|          105|
|     San Diego|          104|
|       Houston|           91|
|         Miami|           87|
|     Las Vegas|           81|
|        Dallas|           75|
|      San Jose|           71|
|       Detroit|           64|
|        Aurora|           64|
|       Phoenix|           64|
|   San Antonio|           53|
|     Lancaster|           52|
|Virginia Beach|           50|
|       Memphis|           48|
+--------------+-------------+
only showing top 20 rows

>>> customersDF.count()
12435

>>> dir(customersDF)
['__class__', '__delattr__', '__dict__', '__doc__', '__format__', '__getattr__', '__getattribute__', '__getitem__', '__hash__', '__init__', '__module__', '__new__',
 '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_collectAsArrow', '_jcols', '_jdf', '_jmap',
 '_jseq', '_lazy_rdd', '_repr_html_', '_sc', '_schema', '_sort_cols', '_support_repr_html', 'agg', 'alias', 'approxQuantile', 'cache', 'checkpoint', 'coalesce',
 'colRegex', 'collect', 'columns', 'corr', 'count', 'cov', 'createGlobalTempView', 'createOrReplaceGlobalTempView', 'createOrReplaceTempView', 'createTempView',
 'crossJoin', 'crosstab', 'cube', 'describe', 'distinct', 'drop', 'dropDuplicates', 'drop_duplicates', 'dropna', 'dtypes', 'exceptAll', 'explain', 'fillna',
 'filter', 'first', 'foreach', 'foreachPartition', 'freqItems', 'groupBy', 'groupby', 'head', 'hint', 'intersect', 'intersectAll', 'isLocal', 'isStreaming',
 'is_cached', 'join', 'limit', 'localCheckpoint', 'na', 'orderBy', 'persist', 'printSchema', 'randomSplit', 'rdd', 'registerTempTable', 'repartition',
 'repartitionByRange', 'replace', 'rollup', 'sample', 'sampleBy', 'schema', 'select', 'selectExpr', 'show', 'sort', 'sortWithinPartitions', 'sql_ctx', 'stat'
 , 'storageLevel', 'subtract', 'summary', 'take', 'toDF', 'toJSON', 'toLocalIterator', 'toPandas', 'union', 'unionAll', 'unionByName', 'unpersist', 'where',
 'withColumn', 'withColumnRenamed', 'withWatermark', 'write', 'writeStream']

>>> from pyspark.sql.types import *       
>>> from pyspark.sql.functions import *
>>> empcollists = [StructField('empno',StringType(),False),StructField('name',StringType(),True),StructField('job',StringType(),True),StructField('mgr',IntegerType(),True),StructField('hiredate',StringType(),True),StructField('sal',IntegerType(),True),StructField('comm',IntegerType(),True),StructField('deptno',StringType(),True)]
>>> empschema = StructType(empcollists)
>>> empDF = spark.read.format('csv').option('header','false').schema(empschema).load('emp1.csv')
>>> empDF.printSchema()
root
 |-- empno: string (nullable = true)
 |-- name: string (nullable = true)
 |-- job: string (nullable = true)
 |-- mgr: integer (nullable = true)
 |-- hiredate: string (nullable = true)
 |-- sal: integer (nullable = true)
 |-- comm: integer (nullable = true)
 |-- deptno: string (nullable = true)

--顯示所有資料
>>> empDF.show()  --SELECT * FROM empDF
+-----+------+---------+----+---------+----+----+------+
|empno|  name|      job| mgr| hiredate| sal|comm|deptno|
+-----+------+---------+----+---------+----+----+------+
| 7839|  KING|PRESIDENT|null|17-NOV-81|5000|null|    10|
| 7698| BLAKE|  MANAGER|7839|01-MAY-81|2850|null|    30|
| 7782| CLARK|  MANAGER|7839|09-JUN-81|2450|null|    10|
| 7566| JONES|  MANAGER|7839|02-APR-81|2975|null|    20|
| 7788| SCOTT|  ANALYST|7566|19-APR-87|3000|null|    20|
| 7902|  FORD|  ANALYST|7566|03-DEC-81|3000|null|    20|
| 7369| SMITH|    CLERK|7902|17-DEC-80| 800|null|    20|
| 7499| ALLEN| SALESMAN|7698|20-FEB-81|1600| 300|    30|
| 7521|  WARD| SALESMAN|7698|22-FEB-81|1250| 500|    30|
| 7654|MARTIN| SALESMAN|7698|28-SEP-81|1250|1400|    30|
| 7844|TURNER| SALESMAN|7698|08-SEP-81|1500|   0|    30|
| 7876| ADAMS|    CLERK|7788|23-MAY-87|1100|null|    20|
| 7900| JAMES|    CLERK|7698|03-DEC-81| 950|null|    30|
| 7934|MILLER|    CLERK|7782|23-JAN-82|1300|null|    10|
+-----+------+---------+----+---------+----+----+------+

>>> empDF.where()
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
TypeError: filter() takes exactly 2 arguments (1 given)

filter(self, condition) method of pyspark.sql.dataframe.DataFrame instance
    :func:`where` is an alias for :func:`filter`.

    .. versionadded:: 1.3

--where()其實是filter(),顯示部分row(以condition當作過濾器)
>>> empDF.where(empDF.sal>5000)
                ----- ---
				   DF.column
DataFrame[empno: string, name: string, job: string, mgr: int, hiredate: string, sal: int, comm: int, deptno: string]

>>> empDF.where(empDF.sal>5000).show()
+-----+----+---+---+--------+---+----+------+
|empno|name|job|mgr|hiredate|sal|comm|deptno|
+-----+----+---+---+--------+---+----+------+
+-----+----+---+---+--------+---+----+------+

>>> empDF.where(empDF.sal>3000).show()
+-----+----+---------+----+---------+----+----+------+
|empno|name|      job| mgr| hiredate| sal|comm|deptno|
+-----+----+---------+----+---------+----+----+------+
| 7839|KING|PRESIDENT|null|17-NOV-81|5000|null|    10|
+-----+----+---------+----+---------+----+----+------+

--select()顯示部分columns
>>> empDF.select(empDF.sal,empDF.deptno).show()
+----+------+
| sal|deptno|
+----+------+
|5000|    10|
|2850|    30|
|2450|    10|
|2975|    20|
|3000|    20|
|3000|    20|
| 800|    20|
|1600|    30|
|1250|    30|
|1250|    30|
|1500|    30|
|1100|    20|
| 950|    30|
|1300|    10|
+----+------+

>>> empDF.where(empDF.sal>3000).select(empDF.empno,empDF.job,empDF.sal).show()
+-----+---------+----+
|empno|      job| sal|
+-----+---------+----+
| 7839|PRESIDENT|5000|
+-----+---------+----+
\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\
--SELECT name,sal FROM empDF; 被optimizer解析後的SQL -> SELECT empDF.name,empDF.sal FROM empDF;
>>> empDF.select(empDF['name'],empDF['sal']).show()
 -- empDF.select('name','sal').show()
 -- empDF.select(empDF.name,empDF.sal).show()
+------+----+
|  name| sal|
+------+----+
|  KING|5000|
| BLAKE|2850|
| CLARK|2450|
| JONES|2975|
| SCOTT|3000|
|  FORD|3000|
| SMITH| 800|
| ALLEN|1600|
|  WARD|1250|
|MARTIN|1250|
|TURNER|1500|
| ADAMS|1100|
| JAMES| 950|
|MILLER|1300|
+------+----+

--expression
>>> empDF.select(empDF.name,empDF.sal*2).show()
+------+---------+
|  name|(sal * 2)|  --欄位名字變成運算式
+------+---------+
|  KING|    10000|
| BLAKE|     5700|
| CLARK|     4900|
| JONES|     5950|
| SCOTT|     6000|
|  FORD|     6000|
| SMITH|     1600|
| ALLEN|     3200|
|  WARD|     2500|
|MARTIN|     2500|
|TURNER|     3000|
| ADAMS|     2200|
| JAMES|     1900|
|MILLER|     2600|
+------+---------+

--column alias
--SELECT empDF.name,empDF.sal*2 salX2 FROM empDF;
>>> empDF.select(empDF.name,(empDF.sal*2).alias('salX2')).show()
+------+-----+
|  name|salX2|
+------+-----+
|  KING|10000|
| BLAKE| 5700|
| CLARK| 4900|
| JONES| 5950|
| SCOTT| 6000|
|  FORD| 6000|
| SMITH| 1600|
| ALLEN| 3200|
|  WARD| 2500|
|MARTIN| 2500|
|TURNER| 3000|
| ADAMS| 2200|
| JAMES| 1900|
|MILLER| 2600|
+------+-----+

>>> empDF1 = empDF.select(empDF['name'],empDF['sal'])
>>> empDF1.show()
+------+----+
|  name| sal|
+------+----+
|  KING|5000|
| BLAKE|2850|
| CLARK|2450|
| JONES|2975|
| SCOTT|3000|
|  FORD|3000|
| SMITH| 800|
| ALLEN|1600|
|  WARD|1250|
|MARTIN|1250|
|TURNER|1500|
| ADAMS|1100|
| JAMES| 950|
|MILLER|1300|
+------+----+

--Add Column
--SELECT name,sal,sal*0.05 income_tax FROM empDF1;
>>> empDF1.select(empDF1.name,empDF1.sal,empDF1.sal*0.05).show()
+------+----+------------+
|  name| sal|(sal * 0.05)|
+------+----+------------+
|  KING|5000|       250.0|
| BLAKE|2850|       142.5|
| CLARK|2450|       122.5|
| JONES|2975|      148.75|
| SCOTT|3000|       150.0|
|  FORD|3000|       150.0|
| SMITH| 800|        40.0|
| ALLEN|1600|        80.0|
|  WARD|1250|        62.5|
|MARTIN|1250|        62.5|
|TURNER|1500|        75.0|
| ADAMS|1100|        55.0|
| JAMES| 950|        47.5|
|MILLER|1300|        65.0|
+------+----+------------+

>>> empDF1.withColumn('income_tax',empDF1['sal']*0.05).show()
+------+----+----------+
|  name| sal|income_tax|
+------+----+----------+
|  KING|5000|     250.0|
| BLAKE|2850|     142.5|
| CLARK|2450|     122.5|
| JONES|2975|    148.75|
| SCOTT|3000|     150.0|
|  FORD|3000|     150.0|
| SMITH| 800|      40.0|
| ALLEN|1600|      80.0|
|  WARD|1250|      62.5|
|MARTIN|1250|      62.5|
|TURNER|1500|      75.0|
| ADAMS|1100|      55.0|
| JAMES| 950|      47.5|
|MILLER|1300|      65.0|
+------+----+----------+

>>> empDF1.printSchema()
root
 |-- name: string (nullable = true)
 |-- sal: integer (nullable = true)

--應用sql function
>>> empDF1.select(length(empDF1.name)).show()
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
NameError: name 'length' is not defined --必須先import sql function
>>> from pyspark.sql.functions import *
>>> empDF1.select(length(empDF1.name)).show()
+------------+
|length(name)|
+------------+
|           4|
|           5|
|           5|
|           5|
|           5|
|           4|
|           5|
|           5|
|           4|
|           6|
|           6|
|           5|
|           5|
|           6|
+------------+
-----------------------------------------------------------------------------------
--如果使用下面方式導入pyspark.sql函數
>>> from pyspark.sql import functions as F
                 明確要求使用pyspark.sql的length
                 --------------------
>>> empDF.select(F.length(empDF.name)).show()
+------------+
|length(name)|
+------------+
|           4|
|           5|
|           5|
|           5|
|           5|
|           4|
|           5|
|           5|
|           4|
|           6|
|           6|
|           5|
|           5|
|           6|
+------------+

===================================================================================================================================================
>>> empDF1['newsal']=empDF1.sal*2
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
TypeError: 'DataFrame' object does not support item assignment
spark DataFrame是特殊形態的rdd(schemaRDD),所以還是繼承RDD不可變更的特性
===================================================================================================================================================
--新增欄位?
>>> empDF2=empDF1.withColumn('newsal',empDF1['sal']*2)
>>> empDF2.show()
+------+----+------+
|  name| sal|newsal|
+------+----+------+
|  KING|5000| 10000|
| BLAKE|2850|  5700|
| CLARK|2450|  4900|
| JONES|2975|  5950|
| SCOTT|3000|  6000|
|  FORD|3000|  6000|
| SMITH| 800|  1600|
| ALLEN|1600|  3200|
|  WARD|1250|  2500|
|MARTIN|1250|  2500|
|TURNER|1500|  3000|
| ADAMS|1100|  2200|
| JAMES| 950|  1900|
|MILLER|1300|  2600|
+------+----+------+

--刪除欄位?
>>> empDF3=empDF2.drop('sal')
>>> empDF3.show()
+------+------+
|  name|newsal|
+------+------+
|  KING| 10000|
| BLAKE|  5700|
| CLARK|  4900|
| JONES|  5950|
| SCOTT|  6000|
|  FORD|  6000|
| SMITH|  1600|
| ALLEN|  3200|
|  WARD|  2500|
|MARTIN|  2500|
|TURNER|  3000|
| ADAMS|  2200|
| JAMES|  1900|
|MILLER|  2600|
+------+------+

>>> empDF3.printSchema()
root
 |-- name: string (nullable = true)
 |-- newsal: integer (nullable = true)
 
>>> from pyspark.sql.functions import *

--SELECT * FROM empDF WHERE sal>2000;
>>> empDF.where(empDF.sal>2000).show()
+-----+-----+---------+----+---------+----+----+------+
|empno| name|      job| mgr| hiredate| sal|comm|deptno|
+-----+-----+---------+----+---------+----+----+------+
| 7839| KING|PRESIDENT|null|17-NOV-81|5000|null|    10|
| 7698|BLAKE|  MANAGER|7839|01-MAY-81|2850|null|    30|
| 7782|CLARK|  MANAGER|7839|09-JUN-81|2450|null|    10|
| 7566|JONES|  MANAGER|7839|02-APR-81|2975|null|    20|
| 7788|SCOTT|  ANALYST|7566|19-APR-87|3000|null|    20|
| 7902| FORD|  ANALYST|7566|03-DEC-81|3000|null|    20|
+-----+-----+---------+----+---------+----+----+------+

--SELECT name,sal FROM empDF where sal>2000;
>>> empDF.where(empDF.sal>2000).select(empDF.name,empDF.sal).show()
+-----+----+
| name| sal|
+-----+----+
| KING|5000|
|BLAKE|2850|
|CLARK|2450|
|JONES|2975|
|SCOTT|3000|
| FORD|3000|
+-----+----+

>>> empDF.where(empDF.name=='king').select(empDF.ename,empDF.sal).show()
+-----+-------+
|ename|    sal|
+-----+-------+
| KING|5000.00|
+-----+-------+

>>> empDF.where(empDF.sal>1000).select(empDF.name,empDF.sal,empDF.deptno).show()
+------+----+------+
|  name| sal|deptno|
+------+----+------+
|  KING|5000|    10|
| BLAKE|2850|    30|
| CLARK|2450|    10|
| JONES|2975|    20|
| SCOTT|3000|    20|
|  FORD|3000|    20|
| ALLEN|1600|    30|
|  WARD|1250|    30|
|MARTIN|1250|    30|
|TURNER|1500|    30|
| ADAMS|1100|    20|
|MILLER|1300|    10|
+------+----+------+
                                            --在python裡,=為指派.等式比較元為==
>>> empDF.where(empDF.sal>1000 and empDF.deptno=10).select(empDF.name,empDF.sal,empDF.deptno).show()
  File "<stdin>", line 1
SyntaxError: keyword can't be an expression
>>> empDF.where(empDF.sal>1000 and empDF.deptno==10).select(empDF.name,empDF.sal,empDF.deptno).show()
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "/opt/cloudera/parcels/SPARK2-2.4.0.cloudera2-1.cdh5.13.3.p0.1041012/lib/spark2/python/pyspark/sql/column.py", line 682, in __nonzero__
    raise ValueError("Cannot convert column into bool: please use '&' for 'and', '|' for 'or', "
ValueError: Cannot convert column into bool: please use '&' for 'and', '|' for 'or', '~' for 'not' when building DataFrame boolean expressions.

--where使用多組條件,AND或OR操作
                                and
>>> empDF.where((empDF.sal>1000) & (empDF.deptno==10)).select(empDF.name,empDF.sal,empDF.deptno).show()
                (條件1) & (條件2)
+------+----+------+
|  name| sal|deptno|
+------+----+------+
|  KING|5000|    10|
| CLARK|2450|    10|
|MILLER|1300|    10|
+------+----+------+    

                                 or
>>> empDF.where((empDF.sal>1000) | (empDF.deptno==10)).select(empDF.name,empDF.sal,empDF.deptno).show()
+------+----+------+
|  name| sal|deptno|
+------+----+------+
|  KING|5000|    10|
| BLAKE|2850|    30|
| CLARK|2450|    10|
| JONES|2975|    20|
| SCOTT|3000|    20|
|  FORD|3000|    20|
| ALLEN|1600|    30|
|  WARD|1250|    30|
|MARTIN|1250|    30|
|TURNER|1500|    30|
| ADAMS|1100|    20|
|MILLER|1300|    10|
+------+----+------+

--SELECT name,sal FROM empDF where sal>2000 ORDER BY sal ASC;
>>> empDF.where(empDF.sal>2000).select(empDF.name,empDF.sal).sort(empDF.sal).show()
+-----+----+
| name| sal|
+-----+----+
|CLARK|2450|
|BLAKE|2850|
|JONES|2975|
| FORD|3000|
|SCOTT|3000|
| KING|5000|
+-----+----+

>>> empDF.where(empDF.sal>2000).select(empDF.name,empDF.sal).sort(empDF.sal,ascending=False).show()
+-----+----+
| name| sal|
+-----+----+
| KING|5000|
|SCOTT|3000|
| FORD|3000|
|JONES|2975|
|BLAKE|2850|
|CLARK|2450|
+-----+----+

--SELECT min(sal) FROM empDF;
>>> empDF.agg({'sal':'min'}).show()  --empDF.agg(min(empDF.sal)).show()
+--------+
|min(sal)|
+--------+
|     800|
+--------+

>>> empDF.agg({'sal':'max'}).show()  --empDF.agg(min(empDF.sal)).show()
+--------+
|max(sal)|
+--------+
|    5000|
+--------+

>>> empDF.agg({'sal':'mean'}).show() --empDF.agg(min(empDF.sal)).show()
+-----------------+
|         avg(sal)|
+-----------------+
|2073.214285714286|
+-----------------+

>>> empDF.agg(max(empDF.sal),min(empDF.sal),avg(empDF.sal),sum(empDF.sal),count(empDF.sal)).show()
+--------+--------+-----------------+--------+----------+
|max(sal)|min(sal)|         avg(sal)|sum(sal)|count(sal)|
+--------+--------+-----------------+--------+----------+
|    5000|     800|2073.214285714286|   29025|        14|
+--------+--------+-----------------+--------+----------+

>>> empDF.groupBy()  --全部的row組成一個group(群)
<pyspark.sql.group.GroupedData object at 0x7fee19cd8ed0>

>>> empDF.groupBy().show()
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
AttributeError: 'GroupedData' object has no attribute 'show'

>>> empDF.printSchema()
root
 |-- empno: string (nullable = true)
 |-- name: string (nullable = true)
 |-- job: string (nullable = true)
 |-- mgr: integer (nullable = true)
 |-- hiredate: string (nullable = true)
 |-- sal: integer (nullable = true)
 |-- comm: integer (nullable = true)
 |-- deptno: string (nullable = true)

>>> empDF.groupby().avg().show()   --對所有數值欄位進行平均值計算
>>> empDF.groupby().avg().show()
+-----------------+-----------------+---------+
|         avg(mgr)|         avg(sal)|avg(comm)|
+-----------------+-----------------+---------+
|7739.307692307692|2073.214285714286|    550.0|
+-----------------+-----------------+---------+

>>> empDF.groupby().avg(empDF.sal).show()
--  empDF.groupby().avg(empDF['sal']).show() 
TypeError: 'Column' object is not callable

--SELECT avg(sal) FROM empDF;
>>> empDF.groupby().avg('sal').show()
+-----------------+
|         avg(sal)|
+-----------------+
|2073.214285714286|
+-----------------+

--依照某個欄位分成多個group
--SELECT deptno,sum(sal) FROM empDF GROUP BY deptno;
>>> empDF.groupby('deptno').sum('sal').show()
+------+--------+
|deptno|sum(sal)|
+------+--------+
|    20|   10875|
|    10|    8750|
|    30|    9400|
+------+--------+

--依照某個欄位分成多個group
--SELECT deptno,avg(sal) FROM empDF GROUP BY deptno;
>>> empDF.groupby('deptno').avg('sal').show()
+------+------------------+
|deptno|          avg(sal)|
+------+------------------+
|    30|1566.6666666666667|
|    20|            2175.0|
|    10|2916.6666666666665|
+------+------------------+

>>> empDF.groupby('deptno','job').round(avg('sal')).show()
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
AttributeError: 'GroupedData' object has no attribute 'round'

>>> empDF_avg = empDF.groupby('deptno').avg('sal')
>>> empDF_avg.show()
+------+------------------+
|deptno|          avg(sal)|
+------+------------------+
|    20|            2175.0|
|    10|2916.6666666666665|
|    30|1566.6666666666667|
+------+------------------+

>>> empDF_avg.select(empDF_avg.deptno,round(empDF_avg['avg(sal)']).alias('avgsal')).show()
+------+------+
|deptno|avgsal|
+------+------+
|    30|1567.0|
|    20|2175.0|
|    10|2917.0|
+------+------+

--依照多個欄位分成多個group
>>> empDF.groupby('deptno','job').avg('sal').show()
  --empDF.groupby(empDF.deptno,empDF.job).avg('sal').show() 
+------+---------+--------+
|deptno|      job|avg(sal)|
+------+---------+--------+
|    20|  ANALYST|  3000.0|
|    20|  MANAGER|  2975.0|
|    30|  MANAGER|  2850.0|
|    30| SALESMAN|  1400.0|
|    30|    CLERK|   950.0|
|    10|PRESIDENT|  5000.0|
|    20|    CLERK|   950.0|
|    10|    CLERK|  1300.0|
|    10|  MANAGER|  2450.0|
+------+---------+--------+

>>> empDF.groupby('deptno','job').agg({'sal':'avg'}).show()
+------+---------+-----------+
|deptno|      job|   avg(sal)|
+------+---------+-----------+
|    10|PRESIDENT|5000.000000|
|    30|    CLERK| 950.000000|
|    10|  MANAGER|2450.000000|
|    30|  MANAGER|2850.000000|
|    20|    CLERK| 950.000000|
|    30| SALESMAN|1400.000000|
|    20|  ANALYST|3000.000000|
|    10|    CLERK|1300.000000|
|    20|  MANAGER|2975.000000|
+------+---------+-----------+

--SELECT deptno,job,count(name),avg(sal) FROM emp GROUP BY deptno,job;
>>> empDF.groupby('deptno','job').agg({'sal':'avg','name':'count'}).show()
+------+---------+------------+-----------+
|deptno|      job| count(name)|   avg(sal)|
+------+---------+------------+-----------+
|    10|PRESIDENT|           1|5000.000000|
|    30|    CLERK|           1| 950.000000|
|    10|  MANAGER|           1|2450.000000|
|    30|  MANAGER|           1|2850.000000|
|    20|    CLERK|           2| 950.000000|
|    30| SALESMAN|           4|1400.000000|
|    20|  ANALYST|           2|3000.000000|
|    10|    CLERK|           1|1300.000000|
|    20|  MANAGER|           1|2975.000000|
+------+---------+------------+-----------+


--User Define Function
https://docs.databricks.com/spark/latest/spark-sql/udf-python.html
--可使用python,scala,java撰寫函數
>>> def squared(s):
...     return s*s
...
>>> spark.udf.register("squaredWithPython", squared)
<function squaredWithPython at 0x7f6fdc463410>
>>> from pyspark.sql.functions import udf
>>> from pyspark.sql.types import LongType

>>> squared_udf = udf(squared, LongType())
>>> empDF.select('name',squared_udf("sal").alias("sal_squared")).show()
+------+-----------+
|  name|sal_squared|
+------+-----------+
|  KING|   25000000|
| BLAKE|    8122500|
| CLARK|    6002500|
| JONES|    8850625|
| SCOTT|    9000000|
|  FORD|    9000000|
| SMITH|     640000|
| ALLEN|    2560000|
|  WARD|    1562500|
|MARTIN|    1562500|
|TURNER|    2250000|
| ADAMS|    1210000|
| JAMES|     902500|
|MILLER|    1690000|
+------+-----------+

\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\
--下載emp3.csv
[cloudera@cdh6 ~]$ curl -O http://10.0.1.100:5050/Class_log/emp3.csv

--請先將emp3.csv上傳到HDFS
[cloudera@cdh6 ~]$ cat emp3.csv
7839,KING,PRESIDENT,,17-NOV-81,5000,,10
7698,BLAKE,MANAGER,7839,01-MAY-81,2850,,30
7782,CLARK,MANAGER,7839,09-JUN-81,2450,,10
7566,JONES,MANAGER,7839,02-APR-81,2975,,20
7788,SCOTT,ANALYST,7566,19-APR-87,3000,,20
7902,FORD,ANALYST,7566,03-DEC-81,3000,,20
7369,SMITH,CLERK,7902,17-DEC-80,800,,20
7499,ALLEN,SALESMAN,7698,20-FEB-81,1600,300,30
7521,WARD,SALESMAN,7698,22-FEB-81,1250,500,30
7654,MARTIN,SALESMAN,7698,28-SEP-81,1250,1400,30
7844,TURNER,SALESMAN,7698,08-SEP-81,1500,0,30
7876,ADAMS,CLERK,7788,23-MAY-87,1100,,20
7900,JAMES,CLERK,7698,03-DEC-81,950,,30
7934,MILLER,CLERK,7782,23-JAN-82,1300,,10
7901,FRANK,TRAINER,7839,23-JAN-85,3300,,50               --與emp.csv,emp1.csv的差異
[cloudera@cdh6 ~]$ hdfs dfs -put emp3.csv
\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\
>>> empcollists = [StructField('empno',StringType(),False),StructField('name',StringType(),True),StructField('job',StringType(),True),StructField('mgr',IntegerType(),True),StructField('hiredate',StringType(),True),StructField('sal',IntegerType(),True),StructField('comm',IntegerType(),True),StructField('deptno',StringType(),True)]
>>> empschema = StructType(empcollists)
>>> empDF = spark.read.format('csv').option('header','false').schema(empschema).load('emp3.csv')  #option('header','false')表示emp1.csv的第一行不是欄位名字,而是一般欄位值
>>> empDF.show()
+-----+------+---------+----+---------+----+----+------+
|empno|  name|      job| mgr| hiredate| sal|comm|deptno|
+-----+------+---------+----+---------+----+----+------+
| 7839|  KING|PRESIDENT|null|17-NOV-81|5000|null|    10|
| 7698| BLAKE|  MANAGER|7839|01-MAY-81|2850|null|    30|
| 7782| CLARK|  MANAGER|7839|09-JUN-81|2450|null|    10|
| 7566| JONES|  MANAGER|7839|02-APR-81|2975|null|    20|
| 7788| SCOTT|  ANALYST|7566|19-APR-87|3000|null|    20|
| 7902|  FORD|  ANALYST|7566|03-DEC-81|3000|null|    20|
| 7369| SMITH|    CLERK|7902|17-DEC-80| 800|null|    20|
| 7499| ALLEN| SALESMAN|7698|20-FEB-81|1600| 300|    30|
| 7521|  WARD| SALESMAN|7698|22-FEB-81|1250| 500|    30|
| 7654|MARTIN| SALESMAN|7698|28-SEP-81|1250|1400|    30|
| 7844|TURNER| SALESMAN|7698|08-SEP-81|1500|   0|    30|
| 7876| ADAMS|    CLERK|7788|23-MAY-87|1100|null|    20|
| 7900| JAMES|    CLERK|7698|03-DEC-81| 950|null|    30|
| 7934|MILLER|    CLERK|7782|23-JAN-82|1300|null|    10|
| 7901| FRANK|  TRAINER|7839|23-JAN-85|3300|null|    50|   --請注意此處
+-----+------+---------+----+---------+----+----+------+

>>> deptcollists = [StructField('deptno',StringType(),False),StructField('dname',StringType(),True),StructField('loc',StringType(),True)]
>>> deptschema = StructType(deptcollists)
>>> deptDF = spark.read.format('csv').option('header','false').schema(deptschema).load('dept1.csv')  --如果出現找不到dept1.csv,請自行上傳dept1.csv到HDFS
>>> deptDF.show()
+------+----------+--------+
|deptno|     dname|     loc|
+------+----------+--------+
|    10|ACCOUNTING|NEW YORK|
|    20|  RESEARCH|  DALLAS|
|    30|     SALES| CHICAGO|
|    40|OPERATIONS|  BOSTON|
+------+----------+--------+

--                                                                ---使用=進行結合,equality join
--  SELECT empDF.*,deptDF.* FROM empDF JOIN deptDF ON (empDF.deptno=deptDF.deptno);
--  empDF.join(deptDF,empDF.deptno==deptDF.deptno,'inner').show()
>>> empDF.join(deptDF,empDF.deptno==deptDF.deptno).show()
+-----+------+---------+----+---------+----+----+------+------+----------+--------+
|empno|  name|      job| mgr| hiredate| sal|comm|deptno|deptno|     dname|     loc|
+-----+------+---------+----+---------+----+----+------+------+----------+--------+
| 7839|  KING|PRESIDENT|null|17-NOV-81|5000|null|    10|    10|ACCOUNTING|NEW YORK|
| 7698| BLAKE|  MANAGER|7839|01-MAY-81|2850|null|    30|    30|     SALES| CHICAGO|
| 7782| CLARK|  MANAGER|7839|09-JUN-81|2450|null|    10|    10|ACCOUNTING|NEW YORK|
| 7566| JONES|  MANAGER|7839|02-APR-81|2975|null|    20|    20|  RESEARCH|  DALLAS|
| 7788| SCOTT|  ANALYST|7566|19-APR-87|3000|null|    20|    20|  RESEARCH|  DALLAS|
| 7902|  FORD|  ANALYST|7566|03-DEC-81|3000|null|    20|    20|  RESEARCH|  DALLAS|
| 7369| SMITH|    CLERK|7902|17-DEC-80| 800|null|    20|    20|  RESEARCH|  DALLAS|
| 7499| ALLEN| SALESMAN|7698|20-FEB-81|1600| 300|    30|    30|     SALES| CHICAGO|
| 7521|  WARD| SALESMAN|7698|22-FEB-81|1250| 500|    30|    30|     SALES| CHICAGO|
| 7654|MARTIN| SALESMAN|7698|28-SEP-81|1250|1400|    30|    30|     SALES| CHICAGO|
| 7844|TURNER| SALESMAN|7698|08-SEP-81|1500|   0|    30|    30|     SALES| CHICAGO|
| 7876| ADAMS|    CLERK|7788|23-MAY-87|1100|null|    20|    20|  RESEARCH|  DALLAS|
| 7900| JAMES|    CLERK|7698|03-DEC-81| 950|null|    30|    30|     SALES| CHICAGO|
| 7934|MILLER|    CLERK|7782|23-JAN-82|1300|null|    10|    10|ACCOUNTING|NEW YORK|
+-----+------+---------+----+---------+----+----+------+------+----------+--------+

empDF部分:| 7901| FRANK|  TRAINER|7839|23-JAN-85|3300|null|    50|並沒有在Inner Join結果顯示出來,因為deptno:50並沒有與deptDF的任何deptno相同
deptDF部分:|    40|OPERATIONS|  BOSTON|並沒有在Inner Join結果顯示出來,因為deptno:40並沒有與empDF的任何deptno相同

--                               ---使用非=進行結合,non-equality join
--  SELECT empDF.*,deptDF.* FROM empDF JOIN deptDF ON (empDF.deptno>deptDF.deptno);
>>> empDF.join(deptDF,empDF.deptno>deptDF.deptno).show()
+-----+------+--------+----+---------+----+----+------+------+----------+--------+
|empno|  name|     job| mgr| hiredate| sal|comm|deptno|deptno|     dname|     loc|
+-----+------+--------+----+---------+----+----+------+------+----------+--------+
| 7698| BLAKE| MANAGER|7839|01-MAY-81|2850|null|    30|    10|ACCOUNTING|NEW YORK|
| 7698| BLAKE| MANAGER|7839|01-MAY-81|2850|null|    30|    20|  RESEARCH|  DALLAS|
| 7566| JONES| MANAGER|7839|02-APR-81|2975|null|    20|    10|ACCOUNTING|NEW YORK|
| 7788| SCOTT| ANALYST|7566|19-APR-87|3000|null|    20|    10|ACCOUNTING|NEW YORK|
| 7902|  FORD| ANALYST|7566|03-DEC-81|3000|null|    20|    10|ACCOUNTING|NEW YORK|
| 7369| SMITH|   CLERK|7902|17-DEC-80| 800|null|    20|    10|ACCOUNTING|NEW YORK|
| 7499| ALLEN|SALESMAN|7698|20-FEB-81|1600| 300|    30|    10|ACCOUNTING|NEW YORK|
| 7499| ALLEN|SALESMAN|7698|20-FEB-81|1600| 300|    30|    20|  RESEARCH|  DALLAS|
| 7521|  WARD|SALESMAN|7698|22-FEB-81|1250| 500|    30|    10|ACCOUNTING|NEW YORK|
| 7521|  WARD|SALESMAN|7698|22-FEB-81|1250| 500|    30|    20|  RESEARCH|  DALLAS|
| 7654|MARTIN|SALESMAN|7698|28-SEP-81|1250|1400|    30|    10|ACCOUNTING|NEW YORK|
| 7654|MARTIN|SALESMAN|7698|28-SEP-81|1250|1400|    30|    20|  RESEARCH|  DALLAS|
| 7844|TURNER|SALESMAN|7698|08-SEP-81|1500|   0|    30|    10|ACCOUNTING|NEW YORK|
| 7844|TURNER|SALESMAN|7698|08-SEP-81|1500|   0|    30|    20|  RESEARCH|  DALLAS|
| 7876| ADAMS|   CLERK|7788|23-MAY-87|1100|null|    20|    10|ACCOUNTING|NEW YORK|
| 7900| JAMES|   CLERK|7698|03-DEC-81| 950|null|    30|    10|ACCOUNTING|NEW YORK|
| 7900| JAMES|   CLERK|7698|03-DEC-81| 950|null|    30|    20|  RESEARCH|  DALLAS|
+-----+------+--------+----+---------+----+----+------+------+----------+--------+
--empDF中,deptno=10的資料都沒有顯示,因為deptno=10不能滿足empDF.deptno>deptDF.deptno

--left_outer,先進行inner join,然後將left(左邊)不滿足join condition的資料顯示出來
>>> empDF.join(deptDF,empDF.deptno==deptDF.deptno,'left_outer').show()
+-----+------+---------+----+---------+----+----+------+------+----------+--------+
|empno|  name|      job| mgr| hiredate| sal|comm|deptno|deptno|     dname|     loc|
+-----+------+---------+----+---------+----+----+------+------+----------+--------+
| 7839|  KING|PRESIDENT|null|17-NOV-81|5000|null|    10|    10|ACCOUNTING|NEW YORK|
| 7698| BLAKE|  MANAGER|7839|01-MAY-81|2850|null|    30|    30|     SALES| CHICAGO|
| 7782| CLARK|  MANAGER|7839|09-JUN-81|2450|null|    10|    10|ACCOUNTING|NEW YORK|
| 7566| JONES|  MANAGER|7839|02-APR-81|2975|null|    20|    20|  RESEARCH|  DALLAS|
| 7788| SCOTT|  ANALYST|7566|19-APR-87|3000|null|    20|    20|  RESEARCH|  DALLAS|
| 7902|  FORD|  ANALYST|7566|03-DEC-81|3000|null|    20|    20|  RESEARCH|  DALLAS|
| 7369| SMITH|    CLERK|7902|17-DEC-80| 800|null|    20|    20|  RESEARCH|  DALLAS|
| 7499| ALLEN| SALESMAN|7698|20-FEB-81|1600| 300|    30|    30|     SALES| CHICAGO|
| 7521|  WARD| SALESMAN|7698|22-FEB-81|1250| 500|    30|    30|     SALES| CHICAGO|
| 7654|MARTIN| SALESMAN|7698|28-SEP-81|1250|1400|    30|    30|     SALES| CHICAGO|
| 7844|TURNER| SALESMAN|7698|08-SEP-81|1500|   0|    30|    30|     SALES| CHICAGO|
| 7876| ADAMS|    CLERK|7788|23-MAY-87|1100|null|    20|    20|  RESEARCH|  DALLAS|
| 7900| JAMES|    CLERK|7698|03-DEC-81| 950|null|    30|    30|     SALES| CHICAGO|
| 7934|MILLER|    CLERK|7782|23-JAN-82|1300|null|    10|    10|ACCOUNTING|NEW YORK|
| 7901| FRANK|  TRAINER|7839|23-JAN-85|3300|null|    50|  null|      null|    null|  --outer join一定發生在inner join完成後
+-----+------+---------+----+---------+----+----+------+------+----------+--------+


--  SELECT empDF.*,deptDF.* FROM empDF RIGHT OUTER JOIN deptDF ON (empDF.deptno=deptDF.deptno);
>>> empDF.join(deptDF,empDF.deptno==deptDF.deptno,'right_outer').show()
+-----+------+---------+----+---------+----+----+------+------+----------+--------+
|empno|  name|      job| mgr| hiredate| sal|comm|deptno|deptno|     dname|     loc|
+-----+------+---------+----+---------+----+----+------+------+----------+--------+
| 7934|MILLER|    CLERK|7782|23-JAN-82|1300|null|    10|    10|ACCOUNTING|NEW YORK|
| 7782| CLARK|  MANAGER|7839|09-JUN-81|2450|null|    10|    10|ACCOUNTING|NEW YORK|
| 7839|  KING|PRESIDENT|null|17-NOV-81|5000|null|    10|    10|ACCOUNTING|NEW YORK|
| 7876| ADAMS|    CLERK|7788|23-MAY-87|1100|null|    20|    20|  RESEARCH|  DALLAS|
| 7369| SMITH|    CLERK|7902|17-DEC-80| 800|null|    20|    20|  RESEARCH|  DALLAS|
| 7902|  FORD|  ANALYST|7566|03-DEC-81|3000|null|    20|    20|  RESEARCH|  DALLAS|
| 7788| SCOTT|  ANALYST|7566|19-APR-87|3000|null|    20|    20|  RESEARCH|  DALLAS|
| 7566| JONES|  MANAGER|7839|02-APR-81|2975|null|    20|    20|  RESEARCH|  DALLAS|
| 7900| JAMES|    CLERK|7698|03-DEC-81| 950|null|    30|    30|     SALES| CHICAGO|
| 7844|TURNER| SALESMAN|7698|08-SEP-81|1500|   0|    30|    30|     SALES| CHICAGO|
| 7654|MARTIN| SALESMAN|7698|28-SEP-81|1250|1400|    30|    30|     SALES| CHICAGO|
| 7521|  WARD| SALESMAN|7698|22-FEB-81|1250| 500|    30|    30|     SALES| CHICAGO|
| 7499| ALLEN| SALESMAN|7698|20-FEB-81|1600| 300|    30|    30|     SALES| CHICAGO|
| 7698| BLAKE|  MANAGER|7839|01-MAY-81|2850|null|    30|    30|     SALES| CHICAGO|
| null|  null|     null|null|     null|null|null|  null|    40|OPERATIONS|  BOSTON|
+-----+------+---------+----+---------+----+----+------+------+----------+--------+

>>> empDF.join(deptDF,empDF.deptno==deptDF.deptno,'full_outer').show()
+-----+------+---------+----+---------+----+----+------+------+----------+--------+
|empno|  name|      job| mgr| hiredate| sal|comm|deptno|deptno|     dname|     loc|
+-----+------+---------+----+---------+----+----+------+------+----------+--------+
| 7698| BLAKE|  MANAGER|7839|01-MAY-81|2850|null|    30|    30|     SALES| CHICAGO|
| 7499| ALLEN| SALESMAN|7698|20-FEB-81|1600| 300|    30|    30|     SALES| CHICAGO|
| 7521|  WARD| SALESMAN|7698|22-FEB-81|1250| 500|    30|    30|     SALES| CHICAGO|
| 7654|MARTIN| SALESMAN|7698|28-SEP-81|1250|1400|    30|    30|     SALES| CHICAGO|
| 7844|TURNER| SALESMAN|7698|08-SEP-81|1500|   0|    30|    30|     SALES| CHICAGO|
| 7900| JAMES|    CLERK|7698|03-DEC-81| 950|null|    30|    30|     SALES| CHICAGO|
| null|  null|     null|null|     null|null|null|  null|    40|OPERATIONS|  BOSTON|
| 7566| JONES|  MANAGER|7839|02-APR-81|2975|null|    20|    20|  RESEARCH|  DALLAS|
| 7788| SCOTT|  ANALYST|7566|19-APR-87|3000|null|    20|    20|  RESEARCH|  DALLAS|
| 7902|  FORD|  ANALYST|7566|03-DEC-81|3000|null|    20|    20|  RESEARCH|  DALLAS|
| 7369| SMITH|    CLERK|7902|17-DEC-80| 800|null|    20|    20|  RESEARCH|  DALLAS|
| 7876| ADAMS|    CLERK|7788|23-MAY-87|1100|null|    20|    20|  RESEARCH|  DALLAS|
| 7839|  KING|PRESIDENT|null|17-NOV-81|5000|null|    10|    10|ACCOUNTING|NEW YORK|
| 7782| CLARK|  MANAGER|7839|09-JUN-81|2450|null|    10|    10|ACCOUNTING|NEW YORK|
| 7934|MILLER|    CLERK|7782|23-JAN-82|1300|null|    10|    10|ACCOUNTING|NEW YORK|
| 7901| FRANK|  TRAINER|7839|23-JAN-85|3300|null|    50|  null|      null|    null|
+-----+------+---------+----+---------+----+----+------+------+----------+--------+

>>> empDF.join(deptDF,empDF.deptno==deptDF.deptno,'cross').show()
+-----+------+---------+----+---------+----+----+------+------+----------+--------+
|empno|  name|      job| mgr| hiredate| sal|comm|deptno|deptno|     dname|     loc|
+-----+------+---------+----+---------+----+----+------+------+----------+--------+
| 7839|  KING|PRESIDENT|null|17-NOV-81|5000|null|    10|    10|ACCOUNTING|NEW YORK|
| 7698| BLAKE|  MANAGER|7839|01-MAY-81|2850|null|    30|    30|     SALES| CHICAGO|
| 7782| CLARK|  MANAGER|7839|09-JUN-81|2450|null|    10|    10|ACCOUNTING|NEW YORK|
| 7566| JONES|  MANAGER|7839|02-APR-81|2975|null|    20|    20|  RESEARCH|  DALLAS|
| 7788| SCOTT|  ANALYST|7566|19-APR-87|3000|null|    20|    20|  RESEARCH|  DALLAS|
| 7902|  FORD|  ANALYST|7566|03-DEC-81|3000|null|    20|    20|  RESEARCH|  DALLAS|
| 7369| SMITH|    CLERK|7902|17-DEC-80| 800|null|    20|    20|  RESEARCH|  DALLAS|
| 7499| ALLEN| SALESMAN|7698|20-FEB-81|1600| 300|    30|    30|     SALES| CHICAGO|
| 7521|  WARD| SALESMAN|7698|22-FEB-81|1250| 500|    30|    30|     SALES| CHICAGO|
| 7654|MARTIN| SALESMAN|7698|28-SEP-81|1250|1400|    30|    30|     SALES| CHICAGO|
| 7844|TURNER| SALESMAN|7698|08-SEP-81|1500|   0|    30|    30|     SALES| CHICAGO|
| 7876| ADAMS|    CLERK|7788|23-MAY-87|1100|null|    20|    20|  RESEARCH|  DALLAS|
| 7900| JAMES|    CLERK|7698|03-DEC-81| 950|null|    30|    30|     SALES| CHICAGO|
| 7934|MILLER|    CLERK|7782|23-JAN-82|1300|null|    10|    10|ACCOUNTING|NEW YORK|
+-----+------+---------+----+---------+----+----+------+------+----------+--------+

--CROSSJOIN
>>> empDF.crossJoin(deptDF).show()
+-----+-----+---------+----+---------+----+----+------+------+----------+--------+
|empno| name|      job| mgr| hiredate| sal|comm|deptno|deptno|     dname|     loc|
+-----+-----+---------+----+---------+----+----+------+------+----------+--------+
| 7839| KING|PRESIDENT|null|17-NOV-81|5000|null|    10|    10|ACCOUNTING|NEW YORK|
| 7839| KING|PRESIDENT|null|17-NOV-81|5000|null|    10|    20|  RESEARCH|  DALLAS|
| 7839| KING|PRESIDENT|null|17-NOV-81|5000|null|    10|    30|     SALES| CHICAGO|
| 7839| KING|PRESIDENT|null|17-NOV-81|5000|null|    10|    40|OPERATIONS|  BOSTON|
| 7698|BLAKE|  MANAGER|7839|01-MAY-81|2850|null|    30|    10|ACCOUNTING|NEW YORK|
| 7698|BLAKE|  MANAGER|7839|01-MAY-81|2850|null|    30|    20|  RESEARCH|  DALLAS|
| 7698|BLAKE|  MANAGER|7839|01-MAY-81|2850|null|    30|    30|     SALES| CHICAGO|
| 7698|BLAKE|  MANAGER|7839|01-MAY-81|2850|null|    30|    40|OPERATIONS|  BOSTON|
| 7782|CLARK|  MANAGER|7839|09-JUN-81|2450|null|    10|    10|ACCOUNTING|NEW YORK|
| 7782|CLARK|  MANAGER|7839|09-JUN-81|2450|null|    10|    20|  RESEARCH|  DALLAS|
| 7782|CLARK|  MANAGER|7839|09-JUN-81|2450|null|    10|    30|     SALES| CHICAGO|
| 7782|CLARK|  MANAGER|7839|09-JUN-81|2450|null|    10|    40|OPERATIONS|  BOSTON|
| 7566|JONES|  MANAGER|7839|02-APR-81|2975|null|    20|    10|ACCOUNTING|NEW YORK|
| 7566|JONES|  MANAGER|7839|02-APR-81|2975|null|    20|    20|  RESEARCH|  DALLAS|
| 7566|JONES|  MANAGER|7839|02-APR-81|2975|null|    20|    30|     SALES| CHICAGO|
| 7566|JONES|  MANAGER|7839|02-APR-81|2975|null|    20|    40|OPERATIONS|  BOSTON|
| 7788|SCOTT|  ANALYST|7566|19-APR-87|3000|null|    20|    10|ACCOUNTING|NEW YORK|
| 7788|SCOTT|  ANALYST|7566|19-APR-87|3000|null|    20|    20|  RESEARCH|  DALLAS|
| 7788|SCOTT|  ANALYST|7566|19-APR-87|3000|null|    20|    30|     SALES| CHICAGO|
| 7788|SCOTT|  ANALYST|7566|19-APR-87|3000|null|    20|    40|OPERATIONS|  BOSTON|
+-----+-----+---------+----+---------+----+----+------+------+----------+--------+
only showing top 20 rows
>>> empDF.count()
15
>>> deptDF.count()
4
>>> empDF.count()*deptDF.count()
60

--多欄位結合(python)                若為scala則使用&&
>>> empDF.join(deptDF,(empDF.deptno==deptDF.deptno) & (empDF.empno> deptDF.deptno)).show()
+-----+------+---------+----+---------+----+----+------+------+----------+--------+
|empno|  name|      job| mgr| hiredate| sal|comm|deptno|deptno|     dname|     loc|
+-----+------+---------+----+---------+----+----+------+------+----------+--------+
| 7839|  KING|PRESIDENT|null|17-NOV-81|5000|null|    10|    10|ACCOUNTING|NEW YORK|
| 7698| BLAKE|  MANAGER|7839|01-MAY-81|2850|null|    30|    30|     SALES| CHICAGO|
| 7782| CLARK|  MANAGER|7839|09-JUN-81|2450|null|    10|    10|ACCOUNTING|NEW YORK|
| 7566| JONES|  MANAGER|7839|02-APR-81|2975|null|    20|    20|  RESEARCH|  DALLAS|
| 7788| SCOTT|  ANALYST|7566|19-APR-87|3000|null|    20|    20|  RESEARCH|  DALLAS|
| 7902|  FORD|  ANALYST|7566|03-DEC-81|3000|null|    20|    20|  RESEARCH|  DALLAS|
| 7369| SMITH|    CLERK|7902|17-DEC-80| 800|null|    20|    20|  RESEARCH|  DALLAS|
| 7499| ALLEN| SALESMAN|7698|20-FEB-81|1600| 300|    30|    30|     SALES| CHICAGO|
| 7521|  WARD| SALESMAN|7698|22-FEB-81|1250| 500|    30|    30|     SALES| CHICAGO|
| 7654|MARTIN| SALESMAN|7698|28-SEP-81|1250|1400|    30|    30|     SALES| CHICAGO|
| 7844|TURNER| SALESMAN|7698|08-SEP-81|1500|   0|    30|    30|     SALES| CHICAGO|
| 7876| ADAMS|    CLERK|7788|23-MAY-87|1100|null|    20|    20|  RESEARCH|  DALLAS|
| 7900| JAMES|    CLERK|7698|03-DEC-81| 950|null|    30|    30|     SALES| CHICAGO|
| 7934|MILLER|    CLERK|7782|23-JAN-82|1300|null|    10|    10|ACCOUNTING|NEW YORK|
+-----+------+---------+----+---------+----+----+------+------+----------+--------+

--明確要求使用broadcast join(deptDF)
>>> empDF.join(deptDF.hint('broadcast'),empDF.deptno==deptDF.deptno).show()
+-----+------+---------+----+---------+----+----+------+------+----------+--------+
|empno|  name|      job| mgr| hiredate| sal|comm|deptno|deptno|     dname|     loc|
+-----+------+---------+----+---------+----+----+------+------+----------+--------+
| 7839|  KING|PRESIDENT|null|17-NOV-81|5000|null|    10|    10|ACCOUNTING|NEW YORK|
| 7698| BLAKE|  MANAGER|7839|01-MAY-81|2850|null|    30|    30|     SALES| CHICAGO|
| 7782| CLARK|  MANAGER|7839|09-JUN-81|2450|null|    10|    10|ACCOUNTING|NEW YORK|
| 7566| JONES|  MANAGER|7839|02-APR-81|2975|null|    20|    20|  RESEARCH|  DALLAS|
| 7788| SCOTT|  ANALYST|7566|19-APR-87|3000|null|    20|    20|  RESEARCH|  DALLAS|
| 7902|  FORD|  ANALYST|7566|03-DEC-81|3000|null|    20|    20|  RESEARCH|  DALLAS|
| 7369| SMITH|    CLERK|7902|17-DEC-80| 800|null|    20|    20|  RESEARCH|  DALLAS|
| 7499| ALLEN| SALESMAN|7698|20-FEB-81|1600| 300|    30|    30|     SALES| CHICAGO|
| 7521|  WARD| SALESMAN|7698|22-FEB-81|1250| 500|    30|    30|     SALES| CHICAGO|
| 7654|MARTIN| SALESMAN|7698|28-SEP-81|1250|1400|    30|    30|     SALES| CHICAGO|
| 7844|TURNER| SALESMAN|7698|08-SEP-81|1500|   0|    30|    30|     SALES| CHICAGO|
| 7876| ADAMS|    CLERK|7788|23-MAY-87|1100|null|    20|    20|  RESEARCH|  DALLAS|
| 7900| JAMES|    CLERK|7698|03-DEC-81| 950|null|    30|    30|     SALES| CHICAGO|
| 7934|MILLER|    CLERK|7782|23-JAN-82|1300|null|    10|    10|ACCOUNTING|NEW YORK|
+-----+------+---------+----+---------+----+----+------+------+----------+--------+


>>> empDF.write.saveAsTable('emp')  #emp目錄位在spark-warehouse之下
[cloudera@cdh6 ~]$ hdfs dfs -ls /user/hive/warehouse/
Found 2 items
drwxr-xr-x   - cloudera supergroup          0 2019-08-12 17:52 /user/hive/warehouse/customers
drwxr-xr-x   - cloudera supergroup          0 2019-08-28 18:48 /user/hive/warehouse/emp
[cloudera@cdh6 ~]$ hdfs dfs -ls /user/hive/warehouse/emp/
Found 2 items
-rw-r--r--   1 cloudera supergroup          0 2019-08-28 18:48 /user/hive/warehouse/emp/_SUCCESS
-rw-r--r--   1 cloudera supergroup       2107 2019-08-28 18:48 /user/hive/warehouse/emp/part-00000-0d452fd6-0bd1-4f85-94ab-b81838a6a6b1-c000.snappy.parquet

[cloudera@cdh6 ~]$ hdfs dfs -getmerge /user/hive/warehouse/emp/ emp.parquet
[cloudera@cdh6 ~]$ parquet-tools schema emp.parquet
message spark_schema {
  optional binary empno (UTF8);
  optional binary name (UTF8);
  optional binary job (UTF8);
  optional int32 mgr;
  optional binary hiredate (UTF8);
  optional int32 sal;
  optional int32 comm;
  optional binary deptno (UTF8);
}

--利用已經存在的Hive Database/Tables
>>> spark.sql('show databases').show()
+------------+
|databaseName|
+------------+
|     default|
+------------+

>>> spark.sql('show tables').show()
+--------+---------+-----------+
|database|tableName|isTemporary|
+--------+---------+-----------+
| default|customers|      false|  --Module01使用sqoop所載入的表格
| default|      emp|      false|  --前面使用empDF.write.saveAsTable('emp')所產生的表格
+--------+---------+-----------+

>>> spark.sql('select name,sal from default.emp').show()
+------+----+
|  name| sal|
+------+----+
|  KING|5000|
| BLAKE|2850|
| CLARK|2450|
| JONES|2975|
| SCOTT|3000|
|  FORD|3000|
| SMITH| 800|
| ALLEN|1600|
|  WARD|1250|
|MARTIN|1250|
|TURNER|1500|
| ADAMS|1100|
| JAMES| 950|
|MILLER|1300|
+------+----+

>>> spark.sql('describe customers').show()
+-----------------+---------+-------+
|         col_name|data_type|comment|
+-----------------+---------+-------+
|      customer_id|      int|   null|
|   customer_fname|   string|   null|
|   customer_lname|   string|   null|
|   customer_email|   string|   null|
|customer_password|   string|   null|
|  customer_street|   string|   null|
|    customer_city|   string|   null|
|   customer_state|   string|   null|
| customer_zipcode|   string|   null|
+-----------------+---------+-------+

>>> spark.sql('select customer_city,count(*) cust_count from default.customers group by customer_city').show()
+---------------+----------+
|  customer_city|cust_count|
+---------------+----------+
|        Hanover|         9|
|         Caguas|      4584|
|         Corona|        25|
|          Tempe|        35|
|  Bowling Green|         8|
|    Springfield|         3|
|  Lawrenceville|        12|
|North Las Vegas|        12|
|       Palatine|         8|
|        Phoenix|        64|
|     Plainfield|        13|
|      Bountiful|         8|
|      Hollywood|        24|
|       Waukegan|         9|
|      Pittsburg|         4|
|      Levittown|         8|
| Mount Prospect|         7|
|       Toa Alta|         3|
|       Brighton|        13|
|     Toms River|        11|
+---------------+----------+
only showing top 20 rows

-->>> spark.sql('select ...').show()
--hive> select ....; 
--共同點:兩者的執行結果都是產生rdd,然後將其submit到spark cluster,所看到的結果不是sql執行結果,而是rdd的執行結果
--不同點:spark.sql使用spark的catalyst optimizer產生rdd
         hive使用calcite optimizer產生rdd

--temporary view
>>> emp.createOrReplaceTempView('emp1')  --emp1並沒有儲存定義於hive metadata
>>> spark.sql('select count(*) from emp1').show()
+--------+
|count(1)|
+--------+
|      14|
+--------+

>>> spark.sql('show tables').show()
+--------+---------+-----------+
|database|tableName|isTemporary|
+--------+---------+-----------+
| default|customers|      false|
| default|      emp|      false|
|        |     emp1|       true|
+--------+---------+-----------+

>>> empDF.createOrReplaceGlobalTempView('emp1GV')
>>> spark.sql('show tables').show()
+--------+---------+-----------+
|database|tableName|isTemporary|
+--------+---------+-----------+
| default|customers|      false|
| default|      emp|      false|
|        |     emp1|       true|
+--------+---------+-----------+

>>> spark.sql('show tables in global_temp').show()
+-----------+---------+-----------+
|   database|tableName|isTemporary|
+-----------+---------+-----------+
|global_temp|   emp1gv|       true|
|           |     emp1|       true|
+-----------+---------+-----------+

>>> spark.sql('select count(*) from emp1GV').show()
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "/opt/cloudera/parcels/CDH-6.3.2-1.cdh6.3.2.p0.1605554/lib/spark/python/pyspark/sql/session.py", line 778, in sql
    return DataFrame(self._jsparkSession.sql(sqlQuery), self._wrapped)
  File "/opt/cloudera/parcels/CDH-6.3.2-1.cdh6.3.2.p0.1605554/lib/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py", line 1257, in __call__
  File "/opt/cloudera/parcels/CDH-6.3.2-1.cdh6.3.2.p0.1605554/lib/spark/python/pyspark/sql/utils.py", line 69, in deco
    raise AnalysisException(s.split(': ', 1)[1], stackTrace)
pyspark.sql.utils.AnalysisException: u'Table or view not found: emp1GV; line 1 pos 21'
>>> spark.sql('select count(*) from global_temp.emp1GV').show()
+--------+
|count(1)|
+--------+
|      15|
+--------+

================================================================================================
hive> desc emp;
OK
empno                   int
name                    string
job                     string
mgr                     int
hiredate                string
sal                     int
comm                    int
deptno                  int
Time taken: 0.05 seconds, Fetched: 8 row(s)
hive> desc formatted emp;
OK
# col_name              data_type               comment

empno                   int
name                    string
job                     string
mgr                     int
hiredate                string
sal                     int
comm                    int
deptno                  int

# Detailed Table Information
Database:               default
Owner:                  cloudera
CreateTime:             Wed Sep 25 21:05:16 PDT 2019
LastAccessTime:         UNKNOWN
Protect Mode:           None
Retention:              0
Location:               hdfs://cdh6:8020/user/hive/warehouse/emp  --saveAsTable('emp')
Table Type:             MANAGED_TABLE
Table Parameters:
        COLUMN_STATS_ACCURATE   false
        numFiles                1
        numRows                 -1
        rawDataSize             -1
        spark.sql.create.version        2.4.0.cloudera2
        spark.sql.sources.provider      parquet
        spark.sql.sources.schema.numParts       1
        spark.sql.sources.schema.part.0 {\"type\":\"struct\",\"fields\":[{\"name\":\"empno\",\"type\":\"integer\",\"nullable\":true,\"metadata\":{}},{\"name\":\"name\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"job\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"mgr\",\"type\":\"integer\",\"nullable\":true,\"metadata\":{}},{\"name\":\"hiredate\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"sal\",\"type\":\"integer\",\"nullable\":true,\"metadata\":{}},{\"name\":\"comm\",\"type\":\"integer\",\"nullable\":true,\"metadata\":{}},{\"name\":\"deptno\",\"type\":\"integer\",\"nullable\":true,\"metadata\":{}}]}
        totalSize               2082
        transient_lastDdlTime   1569470716

# Storage Information
SerDe Library:          org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe
InputFormat:            org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat
OutputFormat:           org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat
Compressed:             No
Num Buckets:            -1
Bucket Columns:         []
Sort Columns:           []
Storage Desc Params:
        path                    hdfs://cdh6:8020/user/hive/warehouse/emp
        serialization.format    1
Time taken: 0.041 seconds, Fetched: 43 row(s)
================================================================================================
>>> spark.sql('select deptno,count(*) empcount from emp group by deptno').show()
+------+--------+
|deptno|empcount|
+------+--------+
|    30|       6|
|    20|       5|
|    10|       3|
+------+--------+

>>> spark.sql('select customer_city,count(*) from customers group by customer_city').show()
+---------------+--------+
|  customer_city|count(1)|
+---------------+--------+
|        Hanover|       9|
|         Caguas|    4584|
|         Corona|      25|
|          Tempe|      35|
|  Bowling Green|       8|
|    Springfield|       3|
|  Lawrenceville|      12|
|North Las Vegas|      12|
|       Palatine|       8|
|        Phoenix|      64|
|     Plainfield|      13|
|      Bountiful|       8|
|      Hollywood|      24|
|       Waukegan|       9|
|      Pittsburg|       4|
|      Levittown|       8|
| Mount Prospect|       7|
|       Toa Alta|       3|
|       Brighton|      13|
|     Toms River|      11|
+---------------+--------+
only showing top 20 rows

>>> spark.sql('select deptno,round(avg(sal)) avgsal from emp group by deptno order by avgsal desc').show()
+------+------+
|deptno|avgsal|
+------+------+
|    10|2917.0|
|    20|2175.0|
|    30|1567.0|
+------+------+


[cloudera@cdh6 ~]$ spark-sql
-bash: spark-sql: command not found  --因為CDH所安裝的spark2並未加入spark-sql介面,請使用Hive